% Rapport de l'étude Homère
@TechReport{homere_2023,
  author = {Pigeon, Caroline and Baltenneck, Nicolas and Galiano, Anna Rita and Uzan, Gérard},
  title  = {Étude Homère - Étude nationale sur la déficience visuelle},
  year   = {2023},
  groups = {Déficience visuelle},
}

@Article{gaunet_verbal_2006,
  author   = {Gaunet, Florence},
  journal  = {Universal Access in the Information Society},
  title    = {Verbal guidance rules for a localized wayfinding aid intended for blind-pedestrians in urban areas},
  year     = {2006},
  issn     = {1615-5289, 1615-5297},
  month    = may,
  number   = {4},
  pages    = {338--353},
  volume   = {4},
  doi      = {10.1007/s10209-003-0086-2},
  groups   = {Description verbale},
  keywords = {Intersection, Outdoor navigation, Verbal route description},
  language = {en},
  url      = {http://link.springer.com/10.1007/s10209-003-0086-2},
  urldate  = {2020-10-21},
}

@Article{gaunet_exploring_2005,
  author     = {Gaunet, Florence and Briffault, Xavier},
  journal    = {Human-Computer Interaction},
  title      = {Exploring the {Functional} {Specifications} of a {Localized} {Wayfinding} {Verbal} {Aid} for {Blind} {Pedestrians}: {Simple} and {Structured} {Urban} {Areas}},
  year       = {2005},
  issn       = {0737-0024},
  month      = sep,
  number     = {3},
  pages      = {267--314},
  volume     = {20},
  doi        = {10.1207/s15327051hci2003_2},
  language   = {en},
  shorttitle = {Exploring the {Functional} {Specifications} of a {Localized} {Wayfinding} {Verbal} {Aid} for {Blind} {Pedestrians}},
  url        = {http://www.tandfonline.com/doi/abs/10.1207/s15327051hci2003_2},
  urldate    = {2020-10-20},
}

@Article{boeing_osmnx_2017,
  author     = {Boeing, Geoff},
  journal    = {Computers, Environment and Urban Systems},
  title      = {{OSMnx}: {New} methods for acquiring, constructing, analyzing, and visualizing complex street networks},
  year       = {2017},
  issn       = {0198-9715},
  month      = sep,
  pages      = {126--139},
  volume     = {65},
  abstract   = {Urban scholars have studied street networks in various ways, but there are data availability and consistency limitations to the current urban planning/street network analysis literature. To address these challenges, this article presents OSMnx, a new tool to make the collection of data and creation and analysis of street networks simple, consistent, automatable and sound from the perspectives of graph theory, transportation, and urban design. OSMnx contributes five significant capabilities for researchers and practitioners: first, the automated downloading of political boundaries and building footprints; second, the tailored and automated downloading and constructing of street network data from OpenStreetMap; third, the algorithmic correction of network topology; fourth, the ability to save street networks to disk as shapefiles, GraphML, or SVG files; and fifth, the ability to analyze street networks, including calculating routes, projecting and visualizing networks, and calculating metric and topological measures. These measures include those common in urban design and transportation studies, as well as advanced measures of the structure and topology of the network. Finally, this article presents a simple case study using OSMnx to construct and analyze street networks in Portland, Oregon.},
  doi        = {10.1016/j.compenvurbsys.2017.05.004},
  groups     = {Manipulation des données},
  keywords   = {OpenStreetMap, Visualization, GIS, Transportation, Complex networks, Python, Resilience, Street network, Urban design, Urban form},
  language   = {en},
  shorttitle = {{OSMnx}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0198971516303970},
  urldate    = {2022-04-08},
}

@Book{ratelle_manuel_2019,
  author    = {Ratelle, Agathe and Couturier, Julie-Anne},
  publisher = {PU Montréal},
  title     = {Manuel d'intervention en orientation et mobilité},
  year      = {2019},
  isbn      = {978-2-7606-3905-8},
  month     = jan,
  abstract  = {Ce manuel unique en français rend compte d'un domaine de pointe très spécialisé encore peu connu dans la population en général. Il aborde l'orientation et la mobilité des personnes non voyantes en présentant les techniques et les habiletés de déplacement ainsi que les nombreuses stratégies d'intervention privilégiées en milieu de réadaptation. Il met également en lumière des processus complexes comme l'audition, la kinésiologie et la psychologie cognitive et se penche sur le rôle important de l'environnement dans l'atteinte de l'autonomie. S'inspirant des pratiques américaines et des ouvrages anglophones les plus actuels, les autrices démontrent leur parfaite connaissance de ces aspects interconnectés essentiels à la pratique professionnelle et enrichissent le sujet par leur propre expertise. Leur savoir prend appui sur une riche bibliographie et sur des aspects importants de la profession : son historique, le rôle du spécialiste et la description des techniques et des étapes de l'enseignement, avec des conseils à la clé et de nombreuses observations tirées de leur expérience.},
  groups    = {Déficience visuelle},
  language  = {Français},
}

@Article{Wabinski2019,
  author    = {Wabiński, Jakub and Mościcka, Albina},
  journal   = {ISPRS International Journal of Geo-Information},
  title     = {Automatic ({Tactile}) {Map} {Generation}—{A} {Systematic} {Literature} {Review}},
  year      = {2019},
  issn      = {2220-9964},
  month     = jul,
  number    = {7},
  pages     = {293},
  volume    = {8},
  abstract  = {This paper presents a systematic literature review that reflects the current state of research in the field of algorithms and models for map generalization, the existing solutions for automatic (tactile) map generation, as well as good practices for designing spatial databases for the purposes of automatic map development. A total number of over 500 primary studies were screened in order to identify the most relevant research on automatic (tactile) map generation from the last decade. The reviewed papers revealed many existing solutions in the field of automatic map production, as well as algorithms (e.g., Douglas–Peucker, Visvalingam–Whyatt) and models (e.g., GAEL, CartACom) for data generalization that might be used to transform traditional spatial data into the haptic form, suitable for blind and visually impaired people. However, it turns out that a comprehensive solution for automatic tactile map generation does not exist.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi       = {10.3390/ijgi8070293},
  file      = {:Wabinski2019 - Automatic (Tactile) Map Generation—A Systematic Literature Review.pdf:PDF;:Wabinski2019 - Automatic (Tactile) Map Generation—A Systematic Literature Review.pdf:PDF},
  groups    = {Cartes tactiles},
  keywords  = {generalization, algorithm, model, map automation, tactile maps, maps for blind and visually impaired},
  language  = {en},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {https://www.mdpi.com/2220-9964/8/7/293},
  urldate   = {2023-07-04},
}

@InProceedings{Minatani2010,
  author    = {Minatani, Kazunori and Watanabe, Tetsuya and Yamaguchi, Toshimitsu and Watanabe, Ken and Akiyama, Joji and Miyagi, Manabi and Oouchi, Susumu},
  booktitle = {Computers {Helping} {People} with {Special} {Needs}},
  title     = {Tactile {Map} {Automated} {Creation} {System} to {Enhance} the {Mobility} of {Blind} {Persons}—{Its} {Design} {Concept} and {Evaluation} through {Experiment}},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  editor    = {Miesenberger, Klaus and Klaus, Joachim and Zagler, Wolfgang and Karshmer, Arthur},
  pages     = {534--540},
  publisher = {Springer},
  series    = {Lecture {Notes} in {Computer} {Science}},
  abstract  = {The authors have developed a tactile map creation system (TMACS). It is intended to assist blind persons’ independent mobility. For this purpose, the system was designed to produce tactile maps, to be manipulated by the blind person and to support producing tactile maps of arbitrary locations of Japan. Through group interviews with blind persons, the authors collected information on what kind of strategies are useful for independent walk and what kind of objects can function as landmarks. TMACS is developed to make good use of these information. From the walking experiment, some assumptions which were made by the authors were confirmed. On the other hand, some unexpected or contradicted results were observed on the usefulness of landmarks and the cause of losing right routes.},
  doi       = {10.1007/978-3-642-14100-3_80},
  file      = {:(Lecture Notes in Computer Science 6180 Information Systems and Applications, incl. Internet_Web, and HCI) Latifa Al-Abdulkarim, Areej Al-Wabil, Maha Al-Yahya, Abeer Al-Humaimeedy (auth.), Klaus Miese.pdf:PDF},
  groups    = {Cartes tactiles},
  isbn      = {9783642141003},
  keywords  = {Blind person, Visually Impaired, Tactile Map, Independent Mobility, Orientation and Mobility},
  language  = {en},
}

@InProceedings{Miele2004,
  author    = {Miele, Joshua A.},
  booktitle = {Proceedings of CSUN International Conference on Technology and Persons with Disabilities},
  title     = {Tactile map automated production (TMAP): Using GIS data to generate braille maps},
  year      = {2004},
  groups    = {Cartes tactiles},
}

@InProceedings{Watanabe2014,
  author    = {Watanabe, Tetsuya and Yamaguchi, Toshimitsu and Koda, Satoko and Minatani, Kazunori},
  booktitle = {Computers {Helping} {People} with {Special} {Needs}},
  title     = {Tactile {Map} {Automated} {Creation} {System} {Using} {OpenStreetMap}},
  year      = {2014},
  address   = {Cham},
  editor    = {Miesenberger, Klaus and Fels, Deborah and Archambault, Dominique and Peňáz, Petr and Zagler, Wolfgang},
  pages     = {42--49},
  publisher = {Springer International Publishing},
  series    = {Lecture {Notes} in {Computer} {Science}},
  abstract  = {We have developed a Web-based tactile map automated creation system tmacs. Users simply type in an address or the name of a building and the system instantly creates an image of a tactile map, which is then printed on capsule paper and raised up by a heater. This time we modified this system to deal with OpenStreetMap (OSM). The advantage of using OSM data is that tmacs becomes to be able to create tactile maps of any location in the world and include information which is useful for blind people such as tactile paving. Another feature of the new system is that sighted users can change the point and scale of a tactile map in the same way as a regular Google Map. We are exploring the possibility of increasing the number of countries whose tactile maps can be created with tmacs.},
  doi       = {10.1007/978-3-319-08599-9_7},
  file      = {:Watanabe2014 - Tactile Map Automated Creation System Using OpenStreetMap.html:URL},
  groups    = {Cartes tactiles},
  isbn      = {9783319085999},
  keywords  = {Blind People, Tactile Map, Tactile Perception, OpenStreetMap, Automated Creation},
  language  = {en},
}

@InProceedings{Cervenka2016,
  author    = {Červenka, Petr and Břinda, Karel and Hanousková, Michaela and Hofman, Petr and Seifert, Radek},
  booktitle = {Computers {Helping} {People} with {Special} {Needs}},
  title     = {Blind {Friendly} {Maps}},
  year      = {2016},
  address   = {Cham},
  editor    = {Miesenberger, Klaus and Bühler, Christian and Penaz, Petr},
  pages     = {131--138},
  publisher = {Springer International Publishing},
  series    = {Lecture {Notes} in {Computer} {Science}},
  abstract  = {Blind people can now use maps located at Mapy.cz, thanks to the long-standing joint efforts of the ELSA Center at the Czech Technical University in Prague, the Teiresias Center at Masaryk University, and the company Seznam.cz. Conventional map underlays are automatically adjusted so that they could be read through touch after being printed on microcapsule paper, which opens a whole new perspective in the use of tactile maps. Users may select an area of their choice in the Czech Republic (only within its boundaries, for the time being) and also the production of tactile maps, including the preparation of the map underlays, takes no more than several minutes.},
  doi       = {10.1007/978-3-319-41267-2_18},
  file      = {:Cervenka2016 - Blind Friendly Maps.html:URL},
  groups    = {Cartes tactiles},
  isbn      = {9783319412672},
  keywords  = {Tactile map, Tactile perception, Blind people, Web maps accessibility, Automated geodata processing},
  language  = {en},
}

@Article{Wabinski2022,
  author     = {Wabiński, Jakub and Mościcka, Albina and Touya, Guillaume},
  journal    = {The Cartographic Journal},
  title      = {Guidelines for {Standardizing} the {Design} of {Tactile} {Maps}: {A} {Review} of {Research} and {Best} {Practice}},
  year       = {2022},
  issn       = {0008-7041},
  month      = jul,
  number     = {3},
  pages      = {239--258},
  volume     = {59},
  abstract   = {Accessibility to tactile maps is limited due to their expensive and time-consuming development. Acceleration of their production requires standardized design guidelines that consider symbol design and production methods. In this paper, based on a review of research and best practice, we summarize knowledge on how to design tactile maps properly and provide a selection of highly legible, recommended symbols for the compilation of tactile maps. We also examine generalization constraints and other design parameters that are necessary for the standardization of tactile mapping. Finally, we explore differences in tactile map design depending upon the selected production method. Over the years, many useful guidelines have been developed although they remain unknown to the wider audience. There is still a long way to go in creating a global standard for the design of tactile maps.},
  doi        = {10.1080/00087041.2022.2097760},
  file       = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1080/00087041.2022.2097760:application/pdf},
  groups     = {Cartes tactiles},
  keywords   = {Tactile maps, design principles, standardization, symbology, production methods, guidelines},
  publisher  = {Taylor \& Francis},
  shorttitle = {Guidelines for {Standardizing} the {Design} of {Tactile} {Maps}},
  url        = {https://doi.org/10.1080/00087041.2022.2097760},
  urldate    = {2023-07-05},
}

@Article{Stampach2016,
  author    = {Štampach, Radim and Mulíčková, Eva},
  journal   = {Journal of Maps},
  title     = {Automated generation of tactile maps},
  year      = {2016},
  issn      = {null},
  month     = nov,
  number    = {sup1},
  pages     = {532--540},
  volume    = {12},
  abstract  = {The Support Centre for Students with Special Needs at Masaryk University (the Teiresias Centre) in Brno, Czech Republic asked the team at the Department of Geography at the Faculty of Science to develop a technological procedure for the preparation of tactile maps from available sources of data. Most users would be blind and visually impaired students of Masaryk University. A set of scripts in Python language was prepared for the ArcGIS environment. The scripts prepare the source data, modify the layers, generate the layout of map sheets and export map sheets. The grid of map sheets covers the whole area of Brno City. Printed map sheets are combinable into larger areas. The scale of each map sheet is 1:2500.},
  doi       = {10.1080/17445647.2016.1196622},
  file      = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1080/17445647.2016.1196622:application/pdf},
  groups    = {Cartes tactiles},
  keywords  = {Tactile map, tyflomap, blind, visually impaired, ArcGIS, Python},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/17445647.2016.1196622},
  urldate   = {2023-07-05},
}

@Article{Touya2019,
  author     = {Touya, Guillaume and Christophe, Sidonie and Favreau, Jean-Marie and Ben Rhaiem, Amine},
  journal    = {International Journal of Cartography},
  title      = {Automatic derivation of on-demand tactile maps for visually impaired people: first experiments and research agenda},
  year       = {2019},
  issn       = {2372-9333},
  month      = jan,
  number     = {1},
  pages      = {67--91},
  volume     = {5},
  abstract   = {Tactile maps are essential tools for visually impaired people to comprehend space and to support the simple pedestrian trips made difficult by their disability. Tactile maps are created manually and printed by specialists, and it takes a large amount of time to create a new one, which prevents using them on demand for everyday use. As a consequence, researchers and cartographers try to automate this creation process, but the existing automated derivation processes do not include generalization or advanced stylization steps, which limits their effectiveness. This paper reports first experiments to include such complex automated cartography processes to provide on-demand tactile maps for visually impaired people. These first experiments were more intended to raise real research issues than solve them, and the paper discusses these issues in a research agenda to achieve automatically derived tactile maps.},
  doi        = {10.1080/23729333.2018.1486784},
  file       = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1080/23729333.2018.1486784:application/pdf},
  groups     = {Cartes tactiles},
  keywords   = {Cartography, tactile map, visual impairment, map generalization, stylization, 3D printing, scale, cognition},
  publisher  = {Taylor \& Francis},
  shorttitle = {Automatic derivation of on-demand tactile maps for visually impaired people},
  url        = {https://doi.org/10.1080/23729333.2018.1486784},
  urldate    = {2023-07-05},
}

@Article{FillieresRiveau2020,
  author    = {Fillières-Riveau, Gauthier and Favreau, Jean-Marie and Barra, Vincent and Touya, Guillaume},
  journal   = {Revue Internationale de Géomatique},
  title     = {Génération de cartes tactiles photoréalistes pour personnes déficientes visuelles par apprentissage profond},
  year      = {2020},
  issn      = {1260-5875, 2116-7060},
  month     = jan,
  number    = {1-2},
  pages     = {105--126},
  volume    = {30},
  abstract  = {Les cartes tactiles photoréalistes sont un des outils mobilisés par les personnes en situation de déficience visuelle pour appréhender leur environnement urbain proche, notamment dans le cadre de la mobilité, pour la traversée de carrefours par exemple. Ces cartes sont aujourd’hui principalement fabriquées artisanalement. Dans cet article, nous proposons une approche permettant de produire une segmentation sémantique d’une imagerie aérienne de précision, étape centrale de cette fabrication. Les différents éléments d’intérêt tels que trottoirs, passages piétons, ou îlots centraux sont ainsi localisés et tracés dans l’espace urbain. Nous présentons en particulier comment l’augmentation de cette imagerie par des données vectorielles issues d’OpenStreetMap permet d’obtenir par une technique d’apprentissage profond (réseau adverse génératif conditionnel) des résultats significatifs. Après avoir présenté les enjeux de ce travail et un état de l’art des techniques existantes, nous détaillons l’approche proposée, et nous étudions les résultats obtenus, en comparant en particulier les segmentations obtenues sans et avec enrichissement par données vectorielles. Les résultats sont très prometteurs.},
  copyright = {© 2020 Lavoisier},
  doi       = {10.3166/rig.2020.00104},
  file      = {Full Text PDF:https\://rig.revuesonline.com/articles/lvrig/pdf/2020/04/rig00104.pdf:application/pdf},
  groups    = {Cartes tactiles},
  language  = {fr},
  publisher = {Lavoisier},
  url       = {https://rig.revuesonline.com/articles/lvrig/abs/2020/04/rig00104/rig00104.html},
  urldate   = {2023-07-05},
}

@Article{Brock2015,
  author    = {Brock, Anke M. and Truillet, Philippe and Oriola, Bernard and Picard, Delphine and Jouffrais, Christophe},
  journal   = {Human–Computer Interaction},
  title     = {Interactivity {Improves} {Usability} of {Geographic} {Maps} for {Visually} {Impaired} {People}},
  year      = {2015},
  issn      = {0737-0024},
  month     = mar,
  number    = {2},
  pages     = {156--194},
  volume    = {30},
  abstract  = {Tactile relief maps are used by visually impaired people to acquire mental representation of space, but they retain important limitations (limited amount of information, braille text, etc.). Interactive maps may overcome these limitations. However, usability of these two types of maps has never been compared. It is then unknown whether interactive maps are equivalent or even better solutions than traditional raised-line maps. This study presents a comparison of usability of a classical raised-line map versus an interactive map composed of a multitouch screen, a raised-line overlay, and audio output. Both maps were tested by 24 blind participants. We measured usability as efficiency, effectiveness, and satisfaction. Our results show that replacing braille with simple audio-tactile interaction significantly improved efficiency and user satisfaction. Effectiveness was not related to the map type but depended on users’ characteristics as well as the category of assessed spatial knowledge. Long-term evaluation of acquired spatial information revealed that maps, whether interactive or not, are useful to build robust survey-type mental representations in blind users. Altogether, these results are encouraging as they show that interactive maps are a good solution for improving map exploration and cognitive mapping in visually impaired people.},
  doi       = {10.1080/07370024.2014.924412},
  file      = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1080/07370024.2014.924412:application/pdf},
  groups    = {Cartes tactiles},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/07370024.2014.924412},
  urldate   = {2023-07-05},
}

@Article{Jiang2023,
  author   = {Jiang, Yuhao and Lobo, Maria-Jesús and Christophe, Sidonie and Jouffrais, Christophe},
  journal  = {AGILE: GIScience Series},
  title    = {Semi-automated {Pipeline} to {Produce} {Customizable} {Tactile} {Maps} of {Street} {Intersections} for {People} with {Visual} {Impairments}},
  year     = {2023},
  month    = jun,
  pages    = {1--8},
  volume   = {4},
  abstract = {Street intersections are very challenging for people with visual impairments. Manually produced tactile maps are an important support in teaching and assisting independent journeys as they can be customized to serve the visually impaired audience with diverse tactile reading and mobility skills in different use scenarios. But the manual map production involves a huge workload that makes the maps less accessible. This paper explores the possibility of semi-automatically producing customizable tactile maps for street intersections. It presents a parameterized semi-automated pipeline based on OSM data that allows the maps to be customized in size, map features, geometry processing choices, and symbolizations. It produces street intersection maps in two scales of three sizes, with different levels of details and styles.},
  doi      = {10.5194/agile-giss-4-29-2023},
  file     = {ResearchGate Link:https\://www.researchgate.net/publication/371392346_Semi-automated_Pipeline_to_Produce_Customizable_Tactile_Maps_of_Street_Intersections_for_People_with_Visual_Impairments:},
  groups   = {Cartes tactiles},
}

@InProceedings{Josselin2016,
  author   = {Josselin, Didier and Roussel, Dorian and Boularouk, Said and Saidi, Abdelberry and Matrouf, Driss and Bonin, Olivier and Altman, Eitan},
  title    = {Sonorous cartography for sighted and blind people},
  year     = {2016},
  month    = jun,
  abstract = {In this article, we test the usability of a cartographic tool mixing maps and sounds. This tool is developed within QuantumGIS as a plugin prototype. We first present some theoretical reflections about synesthesia. Secondly, we explain the way we «sonificate» the images, by associating colors and recorded chords and sounds. Then we present the results of several usability tests in France with different users, including blind people. We finally conclude on the contributions and the limitations of such a tool and draw some perspectives.},
  file     = {:Josselin2016 - Sonorous Cartography for Sighted and Blind People.pdf:PDF},
  groups   = {Cartographie sonore},
  language = {en},
  url      = {https://hal.science/hal-01338081},
  urldate  = {2023-07-12},
}

@Article{Schito2018,
  author     = {Schito, Joram and Fabrikant, Sara Irina},
  journal    = {International Journal of Geographical Information Science},
  title      = {Exploring maps by sounds: using parameter mapping sonification to make digital elevation models audible},
  year       = {2018},
  issn       = {1365-8816},
  month      = may,
  number     = {5},
  pages      = {874--906},
  volume     = {32},
  abstract   = {This study empirically investigates the potential of auditory displays for spatial data exploration, as an additional means to broaden the accessibility and dissemination of geographic information for a diverse body of users. In a mixed factorial experiment, three parameter mapping sonification methods are empirically evaluated to interactively explore discrete and continuous digital elevation models by auditory means. Contrasting prior sonification research, this study’s unique empirical evidence suggests that participants can indeed successfully interpret sonified displays containing continuous spatial data. Specifically, the auditory variable pitch leads to significantly better response accuracy, compared to the sound variable duration. Background and training has a weak effect on data interpretation performance with the auditory display. The more immersive the experienced soundscape, the better participants can interpret the sonified terrain. These encouraging empirical results indeed suggest that interactive auditory displays might offer additional means to disseminate spatial information, and to increase the accessibility to spatial data, beyond the currently dominant visual paradigm.},
  doi        = {10.1080/13658816.2017.1420192},
  file       = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1080/13658816.2017.1420192:application/pdf},
  groups     = {Cartographie sonore},
  keywords   = {Auditory display, parameter mapping sonification, spatial data analysis, digital elevation model, GIS},
  publisher  = {Taylor \& Francis},
  shorttitle = {Exploring maps by sounds},
  url        = {https://doi.org/10.1080/13658816.2017.1420192},
  urldate    = {2023-07-12},
}

@Article{Edler2019,
  author     = {Edler, Dennis and Kühne, Olaf and Keil, Julian and Dickmann, Frank},
  journal    = {KN - Journal of Cartography and Geographic Information},
  title      = {Audiovisual {Cartography}: {Established} and {New} {Multimedia} {Approaches} to {Represent} {Soundscapes}},
  year       = {2019},
  issn       = {2524-4965},
  month      = may,
  number     = {1},
  pages      = {5--17},
  volume     = {69},
  abstract   = {Since the mid-1990s, sound has been discussed and used in multimedia cartography. There are four main variants of auditory map elements that have been established in the theory and practice of audiovisual cartography, i.e. abstract sounds/abstract sound sequences, speech, music and, especially, audiorealistic sequences representing so-called “soundscapes”. In cartography, soundscapes are often addressed in large-scale representations. The term originates in multidisciplinary landscape research. Empirical findings of landscape research, and especially those of social constructivist landscape research, have shown the relevance of non-visual stimuli for people’s individual impressions and meanings of the experienced landscape. Amongst the non-visual landscape dimensions, the auditory dimension is the most prominent one. As 3D cartography offers new methods and techniques of designing and experiencing highly realistic, incl. photo- and audiorealistic landscape representation, this discipline becomes more and more interesting for simulating and presenting multisensory landscapes and for presenting the results of empirical findings in landscape research. After an introduction to traditional means of audiovisual cartography and the relevance of auditory stimuli for social constructivist approaches of landscape research, a modern software-based method is presented which highlights the opportunity to imbed 3D sound data representing a location’s soundscape into audiovisual 3D environments in Virtual Reality (VR). It is technically based on the cross-platform game engine Unity3D.},
  doi        = {10.1007/s42489-019-00004-4},
  file       = {:Edler2019 - Audiovisual Cartography_ Established and New Multimedia Approaches to Represent Soundscapes.pdf:PDF},
  groups     = {Cartographie sonore},
  keywords   = {Audiovisual cartography, Multimedia cartography, Multisensory cartography, Landscapes, Soundscapes, Virtual reality (VR), Unity 3D, Audiovisuelle Kartographie, Multimediakartographie, Multisensorische Kartographie, Landschaft, Klanglandschaft, Virtual Reality (VR), Unity 3D},
  language   = {en},
  shorttitle = {Audiovisual {Cartography}},
  url        = {https://doi.org/10.1007/s42489-019-00004-4},
  urldate    = {2023-07-12},
}

@Article{Guth2019,
  author    = {Guth, David A. and Barlow, Janet M. and Ponchillia, Paul E. and Rodegerdts, Lee A. and Kim, Dae Shik and Lee, Kevin H.},
  journal   = {Transportation Research Record},
  title     = {An {Intersection} {Database} {Facilitates} {Access} to {Complex} {Signalized} {Intersections} for {Pedestrians} with {Vision} {Disabilities}},
  year      = {2019},
  issn      = {0361-1981},
  month     = feb,
  number    = {2},
  pages     = {698--709},
  volume    = {2673},
  abstract  = {A growing number of intersections and crosswalks pose barriers to pedestrians with vision disabilities. This project investigated the effects of providing verbal descriptions of intersections and crosswalks on the performance of street-crossing subtasks by individuals who are totally blind. The authors designed an intersection database containing information relevant to crossing subtasks such as finding and aligning with the crosswalk, deciding when to cross, remaining in the crosswalk, and recognizing the end of a crossing. The authors conducted an experiment with 22 blind adults at two intersections in Portland, Oregon. The intersections included crosswalks that varied widely in geometric and operational characteristics, including the presence or absence of accessibility features. In the no database condition, participants used their typical street-crossing procedures. In the database condition, participants additionally listened to database-generated descriptions of the intersections and crosswalks before crossing. The database descriptions had significant positive effects on some subtasks (primarily “crossing” subtasks such as deciding when to cross) and not others (primarily “wayfinding” subtasks such as remaining in the crosswalk). Participants’ reports of the usefulness of specific features of the database were supported by the empirical findings. Implications of the findings for database development, transportation engineers, blind pedestrians, and orientation and mobility specialists are discussed.},
  doi       = {10.1177/0361198118821673},
  file      = {SAGE PDF Full Text:https\://journals.sagepub.com/doi/pdf/10.1177/0361198118821673:application/pdf},
  groups    = {Spécifique carrefour},
  language  = {en},
  publisher = {SAGE Publications Inc},
  url       = {https://doi.org/10.1177/0361198118821673},
  urldate   = {2023-07-19},
}

@Article{Allen2000,
  author    = {Allen, Gary L.},
  journal   = {Applied Cognitive Psychology},
  title     = {Principles and practices for communicating route knowledge},
  year      = {2000},
  issn      = {1099-0720},
  number    = {4},
  pages     = {333--359},
  volume    = {14},
  abstract  = {A series of experiments was conducted to examine the effect of several principle-based practices hypothesized as being important in communicating route knowledge. Results indicated that remembering and following route directions were facilitated by the practice of (a) presenting the directions in correct temporal–spatial order, consistent with the principle of natural order, (b) concentrating information in statements concerned with choice points, consistent with the principle of referential determinacy, and, to some extent, (c) using spatial designations with which most listeners are facile, consistent with the principle of mutual knowledge. In all studies, women had more difficulty than men in following the route from verbal directions. Possible avenues for explaining this sex-related difference are suggested. Copyright © 2000 John Wiley \& Sons, Ltd.},
  copyright = {Copyright © 2000 John Wiley \& Sons, Ltd.},
  doi       = {10.1002/1099-0720(200007/08)14:4<333::AID-ACP655>3.0.CO;2-C},
  file      = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/1099-0720%28200007/08%2914%3A4%3C333%3A%3AAID-ACP655%3E3.0.CO%3B2-C:application/pdf},
  groups    = {Description verbale},
  language  = {en},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-0720%28200007/08%2914%3A4%3C333%3A%3AAID-ACP655%3E3.0.CO%3B2-C},
  urldate   = {2023-07-20},
}

@InProceedings{Lovelace1999,
  author    = {Lovelace, Kristin L. and Hegarty, Mary and Montello, Daniel R.},
  booktitle = {Spatial {Information} {Theory}. {Cognitive} and {Computational} {Foundations} of {Geographic} {Information} {Science}},
  title     = {Elements of {Good} {Route} {Directions} in {Familiar} and {Unfamiliar} {Environments}},
  year      = {1999},
  address   = {Berlin, Heidelberg},
  editor    = {Freksa, Christian and Mark, David M.},
  pages     = {65--82},
  publisher = {Springer},
  series    = {Lecture {Notes} in {Computer} {Science}},
  abstract  = {Route directions are instructions, primarily verbal, that explain how to get from one place to another. The current study examines several methods for assessing the quality of verbal route directions by characterizing them in terms of the number of elements (such as landmarks, segments or turns) and by subjective ratings of their goodness. Route directions for routes which were both familiar and unfamiliar to the participant were studied. Subjective ratings of the quality of route directions were reliable and consistent across individuals. More complete route directions were rated as being of higher quality. For all routes, inclusion of more segment and turn mentions were correlated with higher quality route directions. Good route descriptions for familiar versus unfamiliar routes differed in terms of the types of landmarks included.},
  doi       = {10.1007/3-540-48384-5_5},
  file      = {:lovelace_elements_1999 - Elements of Good Route Directions in Familiar and Unfamiliar Environments.html:URL},
  groups    = {Description verbale},
  isbn      = {9783540483847},
  keywords  = {Spatial language, Route directions, Familiarity},
  language  = {en},
}

@Article{Denis1992,
  author   = {Denis, Michel and Zimmere, Michel},
  journal  = {Psychological Research},
  title    = {Analog properties of cognitive maps constructed from verbal descriptions},
  year     = {1992},
  issn     = {1430-2772},
  month    = dec,
  number   = {4},
  pages    = {286--298},
  volume   = {54},
  abstract = {Six experiments were conducted to test whether the spatial properties of cognitive maps constructed from verbal descriptions are similar to those of representations derived from visual experience. Data from spatial priming, distance comparison, and mental scanning experiments all suggest that people are able to convert verbal descriptions of configurations into mental representations that reflect the spatial extension of these configurations. In addition, cognitive maps constructed from descriptions exhibit reliable metric properties which make them structurally isomorphic to the corresponding spatial configurations.},
  doi      = {10.1007/BF01358266},
  file     = {:Denis1992 - Analog Properties of Cognitive Maps Constructed from Verbal Descriptions.html:URL},
  groups   = {Description verbale},
  keywords = {Mental Representation, Visual Experience, Spatial Configuration, Verbal Description, Spatial Property},
  language = {en},
  url      = {https://doi.org/10.1007/BF01358266},
  urldate  = {2023-07-26},
}

@Article{Avraamides2004,
  author     = {Avraamides, Marios N. and Loomis, Jack M. and Klatzky, Roberta L. and Golledge, Reginald G.},
  journal    = {Journal of Experimental Psychology. Learning, Memory, and Cognition},
  title      = {Functional equivalence of spatial representations derived from vision and language: evidence from allocentric judgments},
  year       = {2004},
  issn       = {0278-7393},
  month      = jul,
  number     = {4},
  pages      = {804--814},
  volume     = {30},
  abstract   = {Past research (e.g., J. M. Loomis, Y. Lippa, R. L. Klatzky, \& R. G. Golledge, 2002) has indicated that spatial representations derived from spatial language can function equivalently to those derived from perception. The authors tested functional equivalence for reporting spatial relations that were not explicitly stated during learning. Participants learned a spatial layout by visual perception or spatial language and then made allocentric direction and distance judgments. Experiments 1 and 2 indicated allocentric relations could be accurately reported in all modalities, but visually perceived layouts, tested with or without vision, produced faster and less variable directional responses than language. In Experiment 3, when participants were forced to create a spatial image during learning (by spatially updating during a backward translation), functional equivalence of spatial language and visual perception was demonstrated by patterns of latency, systematic error, and variability.},
  doi        = {10.1037/0278-7393.30.4.804},
  file       = {:Avraamides2004 - Functional Equivalence of Spatial Representations Derived from Vision and Language_ Evidence from Allocentric Judgments.html:URL},
  groups     = {Description verbale},
  keywords   = {Female, Humans, Judgment, Language, Learning, Male, Noise, Reaction Time, Space Perception, Visual Perception},
  language   = {eng},
  pmid       = {15238025},
  shorttitle = {Functional equivalence of spatial representations derived from vision and language},
}

@Article{Tinti2006,
  author     = {Tinti, Carla and Adenzato, Mauro and Tamietto, Marco and Cornoldi, Cesare},
  journal    = {Quarterly Journal of Experimental Psychology (2006)},
  title      = {Visual experience is not necessary for efficient survey spatial cognition: evidence from blindness},
  year       = {2006},
  issn       = {1747-0218},
  month      = jul,
  number     = {7},
  pages      = {1306--1328},
  volume     = {59},
  abstract   = {This study investigated whether the lack of visual experience affects the ability to create spatial inferential representations of the survey type. We compared the performance of persons with congenital blindness and that of blindfolded sighted persons on four survey representation-based tasks (Experiment 1). Results showed that persons with blindness performed better than blindfolded sighted controls. We repeated the same tests introducing a third group of persons with late blindness (Experiment 2). This last group performed better than blindfolded sighted participants, whereas differences between participants with late and congenital blindness were nonsignificant. The present findings are compatible with results of other studies, which found that when visual perception is lacking, skill in gathering environmental spatial information provided by nonvisual modalities may contribute to a proper spatial encoding. It is concluded that, although it cannot be asserted that total lack of visual experience incurs no cost, our findings are further evidence that visual experience is not a necessary condition for the development of spatial inferential complex representations.},
  doi        = {10.1080/17470210500214275},
  file       = {:Tinti2006 - Visual Experience Is Not Necessary for Efficient Survey Spatial Cognition_ Evidence from Blindness.html:URL},
  groups     = {Cognition spatiale},
  keywords   = {Adult, Aged, Blindness, Cognition, Female, Humans, Male, Middle Aged, Reaction Time, Space Perception, Visual Perception},
  language   = {eng},
  pmid       = {16769626},
  shorttitle = {Visual experience is not necessary for efficient survey spatial cognition},
}

@Article{Kulyukin2012,
  author     = {Kulyukin, Vladimir and Nicholson, John},
  journal    = {The Open Rehabilitation Journal},
  title      = {Toward {Blind} {Travel} {Support} through {Verbal} {Route} {Directions}: {A} {Path} {Inference} {Algorithm} for {Inferring} {New} {Route} {Descriptions} from {Existing} {Route} {Directions}},
  year       = {2012},
  month      = aug,
  volume     = {5},
  abstract   = {The work presented in this article continues our investigation of such assisted navigation solutions where the main emphasis is placed not on sensor sets or sensor fusion algorithms but on the ability of the travelers to interpret and contextualize verbal route directions en route. This work contributes to our investigation of the research hypothesis that we have formulated and partially validated in our previous studies: if a route is verbally described in sufficient and appropriate amount of detail, independent VI travelers can use their O\&M and problem solving skills to successfully follow the route without any wearable sensors or sensors embedded in the environment. In this investigation, we temporarily put aside the issue of how VI and blind travelers successfully interpret route directions en route and tackle the question of how those route directions can be created, generated, and maintained by online communities. In particular, we focus on the automation of path inference and present an algorithm that may be used as part of the background computation of VGI sites to find new paths in the previous route directions written by online community members, generate new route descriptions from them, and post them for subsequent community editing.},
  doi        = {10.2174/1874943701205010022},
  file       = {Full Text PDF:https\://www.researchgate.net/profile/Vladimir-Kulyukin/publication/267381656_Toward_Blind_Travel_Support_through_Verbal_Route_Directions_A_Path_Inference_Algorithm_for_Inferring_New_Route_Descriptions_from_Existing_Route_Directions/links/5459048b0cf2bccc4912b2fb/Toward-Blind-Travel-Support-through-Verbal-Route-Directions-A-Path-Inference-Algorithm-for-Inferring-New-Route-Descriptions-from-Existing-Route-Directions.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/267381656_Toward_Blind_Travel_Support_through_Verbal_Route_Directions_A_Path_Inference_Algorithm_for_Inferring_New_Route_Descriptions_from_Existing_Route_Directions:},
  groups     = {Description verbale},
  shorttitle = {Toward {Blind} {Travel} {Support} through {Verbal} {Route} {Directions}},
}

@InProceedings{Nicolau2009,
  author     = {Nicolau, Hugo and Jorge, Joaquim and Guerreiro, Tiago},
  booktitle  = {{CHI} '09 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
  title      = {Blobby: how to guide a blind person},
  year       = {2009},
  address    = {New York, NY, USA},
  month      = apr,
  pages      = {3601--3606},
  publisher  = {Association for Computing Machinery},
  series     = {{CHI} {EA} '09},
  abstract   = {For the majority of blind people, walking in unknown places is a very difficult, or even impossible, task to perform, when without help. The adoption of the white cane is the main aid to a blind user's mobility. However, the major difficulties arise in the orientation task. The lack of reference points and the inability to access visual cues are its main causes. We aim to overcome this issue allowing users to walk through unknown places, by receiving a familiar and easily understandable feedback. Our preliminary contributions are in understanding, through user studies, how blind users explore an unknown place, their difficulties, capabilities and needs. We also analyzed how these users create their own mental maps, verbalize a route and communicate with each other. Structuring and generalizing this information, we were able to create a prototype that generates familiar and adequate instructions, behaving like a blind companion, one with similar capabilities that understands his "friend" and speaks the same language. We evaluated the system with the target population, validating our approach and orientation guidelines, while gathering overall user satisfaction.},
  doi        = {10.1145/1520340.1520541},
  groups     = {Description verbale},
  isbn       = {9781605582474},
  keywords   = {accessibility, blind, instructions, evaluation, mobile, familiar, orientation},
  shorttitle = {Blobby},
  url        = {https://doi.org/10.1145/1520340.1520541},
  urldate    = {2023-07-27},
}

@InProceedings{Constantinescu2019,
  author    = {Constantinescu, Angela and Petrausch, Vanessa and Müller, Karin and Stiefelhagen, Rainer},
  booktitle = {Proceedings of the 21st {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
  title     = {Towards a {Standardized} {Grammar} for {Navigation} {Systems} for {Persons} with {Visual} {Impairments}},
  year      = {2019},
  address   = {New York, NY, USA},
  month     = oct,
  pages     = {539--541},
  publisher = {Association for Computing Machinery},
  series    = {{ASSETS} '19},
  abstract  = {Pedestrian navigation systems are rarely accessible or suit the needs of persons with visual impairments. They usually lack a standardized grammar for their speech instructions, forcing users to learn new types of instructions for each new system. Thus, we propose (1) a German grammar with syntax rules and vocabulary for mobile pedestrian navigation systems that take into account the special requirements of people with visual impairments. (2) a set of rules for specifying what should be spoken and when, given GPS accuracy in a city [18]. We describe (3) the methodology used to obtain the grammar as well as (4) a qualitative evaluation with orientation and mobility experts and with people with visual impairments who deployed our grammar during a user study. Our approach is the first of its kind, as there is no such grammar neither for German nor for English, as far as we know. It serves as a contribution to standardize pedestrian navigation speech instructions for people with visual impairments.},
  doi       = {10.1145/3308561.3354618},
  groups    = {Description verbale},
  isbn      = {9781450366762},
  keywords  = {mobile pedestrian navigation, grammar, german language, visual impairments, syntax},
  url       = {https://doi.org/10.1145/3308561.3354618},
  urldate   = {2023-07-27},
}

@InCollection{Troeger2020,
  author    = {Tröger, Johannes and Schnebelt, Sarah and Alexandersson, Jan},
  publisher = {Springer International Publishing},
  title     = {Modelling the {Creation} of {Verbal} {Indoor} {Route} {Descriptions} for {Visually} {Impaired} {Travellers}},
  year      = {2020},
  address   = {Cham},
  editor    = {Paiva, Sara},
  isbn      = {9783030164508},
  pages     = {355--377},
  series    = {{EAI}/{Springer} {Innovations} in {Communication} and {Computing}},
  abstract  = {Within the field of computer-supported indoor navigation for visually impaired people, the generation of effective verbal route descriptions and directions to be given to the visually impaired person remains challenging. This paper provides a formal and innovative model for the creation of indoor verbal route descriptions (VRDs) with the iterative methodologies from user-centred design, focused on those with visual impairment (VI), and emphasising sufficient evaluation. As one step towards fully automated generation of verbal directions, four sighted persons were tasked with generating VRDs for two routes at Saarbrücken University. The generated VRDs were evaluated with feedback by 11 VIs. This was used to improve the VRDs which were evaluated again by 3 VIs, with the set of 7 directions having the overall result rated as medium. Implications and main pitfalls of the current model are discussed.},
  doi       = {10.1007/978-3-030-16450-8_15},
  file      = {:Troeger2020 - Modelling the Creation of Verbal Indoor Route Descriptions for Visually Impaired Travellers.html:URL},
  groups    = {Description verbale},
  keywords  = {Indoor navigation, Verbal route description, Visually impaired, User-centred design, Modelling navigation descriptions},
  language  = {en},
  url       = {https://doi.org/10.1007/978-3-030-16450-8_15},
  urldate   = {2023-07-27},
}

@InProceedings{Kulyukin2008,
  author     = {Kulyukin, Vladimir and Nicholson, John and Coster, Daniel},
  title      = {{ShopTalk}: {Toward} independent shopping by people with visual impairments},
  year       = {2008},
  month      = oct,
  pages      = {241--242},
  abstract   = {ABSTRACT ShopTalk, a proof-of-concept system designed to assist indi- viduals with visual impairments with finding shelved prod- ucts in grocery stores, is built on the assumption that sim- ple verbal route directions and layout descriptions can be used to leverage the O\&M skills of independent visually im- paired travelers to enable them,to navigate the store and retrieve shelved products. This paper introduces ShopTalk and summarizes experiments performed in a real-world su- permarket. Categories and Subject Descriptors K.4 [Computers and Society]: Social Issues—Assistive technologies for persons with disabilities General Terms},
  doi        = {10.1145/1414471.1414518},
  file       = {Full Text PDF:https\://www.researchgate.net/profile/Vladimir-Kulyukin/publication/221652189_ShopTalk_Toward_independent_shopping_by_people_with_visual_impairments/links/0c960530cc8e4bbf71000000/ShopTalk-Toward-independent-shopping-by-people-with-visual-impairments.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/221652189_ShopTalk_Toward_independent_shopping_by_people_with_visual_impairments:},
  groups     = {Description verbale},
  shorttitle = {{ShopTalk}},
}

@InProceedings{Balata2016,
  author    = {Balata, Jan and Mikovec, Zdenek and Bures, Petr and Mulickova, Eva},
  booktitle = {2016 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
  title     = {Automatically generated landmark-enhanced navigation instructions for blind pedestrians},
  year      = {2016},
  month     = sep,
  pages     = {1605--1612},
  abstract  = {Visual impairment limits a person mainly in ability to move freely and independently. Even with many navigation aids and tools currently on the market, almost one third of the visually impaired do not travel independently without a guide, and human-prepared landmark-enhanced itineraries of the route are the most useful. We designed a system which based on a specific efficiently collected geographical data generates human-like landmark-enhanced navigation instructions. The studies we conducted (quantitative n = 16, qualitative n = 6) proved usability and efficiency of the system. Further we provide set of design recommendations to increase the usability of the system along with specific examples of usage with particular landmarks.},
  groups    = {Description verbale},
  keywords  = {Navigation, Measurement, Geographic information systems, Urban areas, Visualization, Routing, Buildings},
}

@InCollection{Krygier1994,
  author    = {Krygier, JOHN B.},
  publisher = {Academic Press},
  title     = {Chapter 8 - {Sound} and {Geographic} {Visualization}},
  year      = {1994},
  editor    = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
  month     = jan,
  pages     = {149--166},
  series    = {Visualization in {Modern} {Cartography}},
  volume    = {2},
  doi       = {10.1016/B978-0-08-042415-6.50015-6},
  file      = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/B9780080424156500156/pdfft?isDTMRedir=true&download=true:application/pdf},
  groups    = {Cartographie sonore},
  language  = {en},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780080424156500156},
  urldate   = {2023-08-03},
}

@Article{Porteous1985,
  author    = {Porteous, J. Douglas and Mastin, Jane F.},
  journal   = {Journal of Architectural and Planning Research},
  title     = {Soundscape},
  year      = {1985},
  issn      = {0738-0895},
  number    = {3},
  pages     = {169--186},
  volume    = {2},
  abstract  = {Soundscape is defined as the overall sonic environment of an area. The soundscape of the South Fairfield urban neighborhood (Victoria, British Columbia, Canada) was investigated and regionalized objectively (by machine recording and analysis and expert listening) and subjectively (by means of a survey of 62 residents). Traffic was the most ubiquitous ground sound, had the strongest positive relationship with sound pressure level, occasionally masked figure or keynote sounds, and was usually negatively perceived. Natural sounds were most preferred. Results suggest that urban residents have low levels of awareness of soundscape and that the experience of modern urban life involves a high degree of sensory privation. (20 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address   = {US},
  groups    = {Cartographie sonore},
  keywords  = {Auditory Stimulation, Environment, Urban Environments, Neighborhoods},
  publisher = {Locke Science Publishing},
}

@Article{Bearman2010,
  author    = {Bearman, Nick and Lovett, Andrew},
  journal   = {The Cartographic Journal},
  title     = {Using {Sound} to {Represent} {Positional} {Accuracy} of {Address} {Locations}},
  year      = {2010},
  issn      = {0008-7041},
  month     = nov,
  number    = {4},
  pages     = {308--314},
  volume    = {47},
  abstract  = {title/{\textgreater}Uncertainty data are often ignored by spatial data users for reasons that include difficulty of representation and comprehension. This study evaluated the benefits of a sonification extension to ArcGIS which represented the positional accuracy of address locations using piano notes. The approach was assessed by 49 survey participants via a computer-based task and subsequent discussion. Two factors that influenced successful interpretation were the proportion of values requiring detection and the presentation method. Knowledge of the data source also appeared relevant. Future studies will examine applications to climate scenarios and visualisations of future landscapes, as well as other aspects of sound.},
  doi       = {10.1179/000870410X12911302296833},
  file      = {Full Text PDF:https\://www.tandfonline.com/doi/pdf/10.1179/000870410X12911302296833:application/pdf},
  groups    = {Cartographie sonore},
  keywords  = {positional accuracy, sound, sonification, tone, ArcGIS},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1179/000870410X12911302296833},
  urldate   = {2023-08-03},
}

@Article{Foteinou2022,
  author     = {Foteinou, Aikaterini and Kokla, Margarita and Tomai, Eleni and Kavouras, Marinos},
  journal    = {AGILE: GIScience Series},
  title      = {Sonification of {Spatial} {Data}: {An} {Online} {Audiovisual} {Cartographic} {Representation} of {Fire} {Incidents}},
  year       = {2022},
  issn       = {2700-8150},
  month      = jun,
  note       = {ADS Bibcode: 2022AGILE...3...35F},
  pages      = {35},
  volume     = {3},
  abstract   = {The possibilities of using sound in cartography have been formulated by numerous researchers. However, there are still no general guidelines for mapping data dimensions to auditory variables, while the decision of which spatial data dimension to represent by which sound variable is crucial. The method for embedding sound in maps is commonly known as "sonification"; the representation of data through sound. Many researchers use sonification to convey their data through the auditory channel as an alternative way to understand and represent our complicated world. With this in mind, we created an interactive web map that depicts the fire dynamics, adopting the sonification technique of parameter mapping; a sound variable was used to represent fire duration. For assessing the effectiveness of different sound variables for this map, an online survey was conducted. The main finding is that to represent spatial data through sound, participatory approaches can highlight the most effective cross-modal correspondence.},
  doi        = {10.5194/agile-giss-3-35-2022},
  file       = {:Foteinou2022 - Sonification of Spatial Data_ an Online Audiovisual Cartographic Representation of Fire Incidents.pdf:PDF},
  groups     = {Cartographie sonore},
  shorttitle = {Sonification of {Spatial} {Data}},
  url        = {https://ui.adsabs.harvard.edu/abs/2022AGILE...3...35F},
  urldate    = {2023-08-04},
}

@Article{Miele2006,
  author     = {Miele, Joshua A. and Landau, Steven and Gilden, Deborah},
  journal    = {British Journal of Visual Impairment},
  title      = {Talking {TMAP}: {Automated} generation of audio-tactile maps using {Smith}-{Kettlewell}'s {TMAP} software},
  year       = {2006},
  issn       = {0264-6196},
  month      = may,
  number     = {2},
  pages      = {93--100},
  volume     = {24},
  abstract   = {Traditional tactile cartography is complicated by problems associated with braille labeling and feature annotation. Audio-tactile display techniques can address many of these issues by associating spoken information and sounds with specific map elements. This article introduces Talking TMAP – a collaborative effort between The Smith-Kettlewell Eye Research Institute and Touch Graphics, Inc. Talking TMAP combines existing tools such as the World Wide Web, geographic information systems, braille embossers and touch tablet technology in new ways to produce a system capable of creating detailed and accurate audio-tactile street maps of any neighborhood. The article describes software design, user interface and plans for future implementation.},
  doi        = {10.1177/0264619606064436},
  file       = {SAGE PDF Full Text:https\://journals.sagepub.com/doi/pdf/10.1177/0264619606064436:application/pdf},
  groups     = {Cartes tactiles, Cartes audio-tactiles},
  language   = {en},
  publisher  = {SAGE Publications Ltd},
  shorttitle = {Talking {TMAP}},
  url        = {https://doi.org/10.1177/0264619606064436},
  urldate    = {2023-07-05},
}

@InProceedings{Loetzsch1994,
  author    = {Lötzsch, Jürgen},
  booktitle = {Computers for {Handicapped} {Persons}},
  title     = {Computer-aided access to tactile graphics for the blind},
  year      = {1994},
  address   = {Berlin, Heidelberg},
  editor    = {Zagler, Wolfgang L. and Busby, Geoffrey and Wagner, Roland R.},
  pages     = {575--581},
  publisher = {Springer},
  series    = {Lecture {Notes} in {Computer} {Science}},
  abstract  = {Every day sighted people use 2D graphic representations of informations for communication and documentation. At school and work but also in the daily life they bring a lot of advantages especially efficiency and speed. Now we see all of them on computer screens. But how can blind people manage the access to such 2D representations used in computers ? The experiences, gained during the author's involvement in projects dealing with audio-tactile pictures, are presented in this paper.},
  doi       = {10.1007/3-540-58476-5_188},
  file      = {:Loetzsch1994 - Computer Aided Access to Tactile Graphics for the Blind.pdf:PDF},
  groups    = {Cartes audio-tactiles},
  isbn      = {9783540489894},
  keywords  = {Blind People, Graphic Object, Sound Card, Speech Synthesizer, Audio Channel},
  language  = {en},
}

@Article{Landau2001,
  author   = {Landau, Steven and Gourgey, K.},
  journal  = {Information Technology and Disabilities},
  title    = {Development of a {Talking} {Tactile} {Tablet}},
  year     = {2001},
  month    = apr,
  abstract = {Researchers have long understood the value of tactile presentation of pictures, maps and diagrams for readers who are blind or otherwise visually impaired (Edman, 1992). However, some practicalities have always limited the usefulness and appeal of these materials. It is often difficult for a blind individual to make sense of tactile shapes and textures without some extra information to confirm or augment what has been touched (Kennedy, Tobias \& Nichols, 1991). Labeling a drawing with Braille is one way to accomplish this, but since Braille tags must be large and have plenty of blank space around them to be legible, they are not ideal for use with fairly complex or graphically rich images. Also, reliance on Braille labeling restricts the usefulness of tactile graphics to those blind or visually impaired persons who are competent Braille readers, a lamentably small population. In order to enrich the tactile graphic experience and to allow for a broader range of users, products like NOMAD have been created. NOMAD was developed by Dr. Don Parkes of the University of New South Wales, and was first brought to market in 1989 (Parkes, 1994). When connected to a computer, NOMAD promised to enhance the tactile experience by allowing a user to view pictures, graphs, diagrams etc, and then to press on various features to hear descriptions, labels and other explanatory audio material. Further, the NOMAD aspired to offer the kind of multi-media, interactive experiences that have exploded on the scene in visual computing. Several factors, however, have always prevented the NOMAD system from being widely adopted. Resolution of the touch-sensitive surface is low, so precise correspondence of graphic images and audio tags is difficult to achieve. Speech is synthetically produced, so it is not as clear and "user friendly" as pre-recorded human speech that is common in mainstream CD ROMs. Perhaps most importantly, high quality interactive programming and tactile media was not independently produced in sufficient quantities to justify the hardware investment, as was envisioned by the NOMAD creators. Given the immense promise of audio-tactile strategies to open interactive learning and entertainment to those whose vision problems preclude their use of a mouse or a video monitor, Touch Graphics was established in 1997. This for-profit company, created in cooperation with Baruch College's Computer Center for Visually Impaired People, has successfully competed for research and development funds through the Small Business Innovation Research programs at the US Department of Education and the National Science Foundation. The work has led, thus far, to the creation of a prototype Talking Tactile Tablet (TTT) and three interactive programs for use with the device. The hardware component of the TTT system is an extremely simple, durable and inexpensive "easel" (fig. 1) for holding tactile graphic sheets motionless against a high-resolution touch-sensitive surface. A user's finger pressure is transmitted through a variety of flexible tactile graphic overlays to this surface, which is a standard hardened-glass touch screen, typically used in conjunction with a video monitor for ATMs and other applications. The TTT device is connected to a host computer by a single cable that is plugged into the USB port on a Macintosh or Windows-based computer. Alternatively, a completely free-standing version (fig. 2) has been created, in which the tablet is set into a "docking station" that incorporates a Single Board Computer, a hard disk drive, audio system, and connections to peripheral devices. In both cases, the computer interprets the user's presses on the tactile graphic overlay sheet in exactly the same way that it does when a mouse is clicked while the cursor is over a particular region, icon or other object on a video screen. With appropriate software, the system promises to open the world of "point and click" computing to blind and visually impaired users. …},
  annote   = {[TLDR] Touch Graphics was established in 1997 and has successfully competed for research and development funds through the Small Business Innovation Research programs at the US Department of Education and the National Science Foundation, and has led to the creation of a prototype Talking Tactile Tablet (TTT) and three interactive programs for use with the device.},
  file     = {:Landau2001 - Development of a Talking Tactile Tablet.html:URL},
  groups   = {Cartes audio-tactiles},
  url      = {https://www.semanticscholar.org/paper/Development-of-a-Talking-Tactile-Tablet-Landau-Gourgey/46b57cad3779295df806d53999619465ba8b2024},
  urldate  = {2023-08-04},
}

@Book{AschanLeygonie2019,
  author    = {Aschan-Leygonie, Christina and Cunty, Claire and Davoine, Paule-Annick},
  publisher = {{Armand Colin}},
  title     = {Les systèmes d'information géographique},
  year      = {2019},
  isbn      = {978-2-200-61718-9},
  month     = oct,
  abstract  = {Cet ouvrage apporte aux étudiants les notions clés et méthodes indispensables à la maîtrise des systèmes d’information géographique (SIG). Il présente les fondamentaux de l’information géographique, les traitements et les analyses qui sont au coeur des projets SIG. Il montre comment les SIG peuvent répondre à des problématiques spatiales portant aussi bien sur des phénomènes géographiques discrets ou continus que sociaux ou environnementaux.Un véritable outil pédagogique, illustré par de multiples exemples, une centaine de figures, des focus thématiques, des conseils, des définitions et des questions de révision.},
  groups    = {Données géographiques},
  langid    = {french},
  pagetotal = {272},
}

@InProceedings{Palacio2003,
  author     = {Palacio, M.P. and Sol, D. and Gonzalez, J.},
  booktitle  = {Proceedings of the {{Fourth Mexican International Conference}} on {{Computer Science}}, 2003. {{ENC}} 2003.},
  title      = {Graph-Based Knowledge Representation for {{GIS}} Data},
  year       = {2003},
  month      = sep,
  pages      = {117--124},
  abstract   = {This paper presents a proposal to create a graph representation for GIS, using both spatial and non-spatial data and also including spatial relations between spatial objects. Because graphs are a powerful and flexible knowledge representation we are able to combine spatial and non-spatial data at the same time and this is one of the strengths of the proposal. We hope to apply this knowledge representation to the data mining process with GIS data including three types of spatial relations: topological, orientation and distance.},
  doi        = {10.1109/ENC.2003.1232884},
  eventtitle = {Proceedings of the {{Fourth Mexican International Conference}} on {{Computer Science}}, 2003. {{ENC}} 2003.},
  file       = {/Users/jeremy/Zotero/storage/IAUI6A94/1232884.html},
  groups     = {Données géographiques},
  keywords   = {Data analysis,Data mining,Explosives,Geographic Information Systems,Humans,Knowledge representation,Multidimensional systems,Proposals,Relational databases,Spatial databases},
}

@InProceedings{Ding2014,
  author    = {Ding, Chaohai and Wald, Mike and Wills, Gary},
  booktitle = {Proceedings of the 11th {{Web}} for {{All Conference}}},
  title     = {A Survey of Open Accessibility Data},
  year      = {2014},
  address   = {{New York, NY, USA}},
  month     = apr,
  pages     = {1--4},
  publisher = {{Association for Computing Machinery}},
  series    = {{{W4A}} '14},
  abstract  = {This paper presents the research of using Linked Data for enhancing accessibility data, especially for accessible travelling. With the aim of addressing the gap between users' special needs and accessibility data, this research initially explores the current situation of open accessibility data. Open accessibility data is the data related to the accessibility issues and associated with geographical data, which could benefit people with disabilities or special needs. This paper proposed a survey of open accessibility data in UK based on the datasets retrieved from five different resources. After examining the features of each dataset, a mapping approach using Semantic Web technologies is proposed to interlink these datasets together to generate a linked open accessibility repository and link this repository to other resources on the Linked Open Data Cloud (LODC). As a result, this research would not only benefit people with disabilities, but also contribute to a novel method to address accessibility information barriers by establishing a linked open accessibility data repository for publishing, integrating and consuming the accessibility data.},
  doi       = {10.1145/2596695.2596708},
  file      = {/Users/jeremy/Zotero/storage/7ZL9QYHL/Ding et al. - 2014 - A survey of open accessibility data.pdf},
  groups    = {Données d'accessibilité},
  isbn      = {978-1-4503-2651-3},
  keywords  = {data interlinking,information retrieval,linked data,open accessibility data},
  url       = {https://doi.org/10.1145/2596695.2596708},
  urldate   = {2023-08-15},
}

@Report{CEREMA2022,
  author = {CEREMA},
  file   = {/Users/jeremy/Zotero/storage/ZWH6AICI/CEREMA - 2022 - Les cheminements des personnes aveugles et malvoya.pdf},
  groups = {Données d'accessibilité},
  langid = {french},
  month  = jan,
  pages  = {86},
  title  = {Les cheminements des personnes aveugles et malvoyantes},
  year   = {2022},
}

@Article{Beale2006,
  author     = {Beale, Linda and Field, Kenneth and Briggs, David and Picton, Phil and Matthews, Hugh},
  journal    = {The Cartographic Journal},
  title      = {Mapping for {{Wheelchair Users}}: {{Route Navigation}} in {{Urban Spaces}}},
  year       = {2006},
  issn       = {0008-7041},
  month      = mar,
  number     = {1},
  pages      = {68--81},
  volume     = {43},
  abstract   = {Navigation around urban areas is often constraining for the mobility-impaired due to the fabric of the urban landscape, and there is a need to provide maps tailored to individual abilities. Barriers to effective navigation, such as slope, surface type and dropped kerbs, differ for able-bodied pedestrians and wheelchair users. This study identifies and quantifies such differences, and develops a Geographical Information Systems (GIS) network model for the creation of accessibility maps for wheelchair users. The measurement of barriers uses Digital Elevation Models, calculation of rolling resistance, and surveys in the field using hand-held GIS. A spatial database has been constructed which contains the pedestrian route network and barriers to navigation. A GIS application runs the model, providing a user-friendly interface to define and calculate routes through the pedestrian route network that take account of impedances to accessibility. The model, application and interface has been tested with wheelchair users and the route selection provides a good correspondence with patterns of route finding already established through experience. The interface and individually tailored maps generated, provide a tool suitable to assist wheelchair users new to an area; to enable better navigation for existing users, and a means for planners to consider the way in which access is restricted for wheelchair users in their designs for more inclusive urban environments.},
  doi        = {10.1179/000870406X93517},
  groups     = {Données d'accessibilité},
  keywords   = {ACCESS,DYNAMIC SEGMENTATION,GIS,MAPPING,MODELLING,NETWORK,URBAN},
  publisher  = {{Taylor \& Francis}},
  shorttitle = {Mapping for {{Wheelchair Users}}},
  url        = {https://doi.org/10.1179/000870406X93517},
  urldate    = {2023-08-15},
}

@Article{Goodchild2007,
  author       = {Goodchild, Michael F.},
  journal      = {GeoJournal},
  title        = {Citizens as Sensors: The World of Volunteered Geography},
  year         = {2007},
  issn         = {0343-2521, 1572-9893},
  month        = nov,
  number       = {4},
  pages        = {211--221},
  volume       = {69},
  doi          = {10.1007/s10708-007-9111-y},
  file         = {/Users/jeremy/Zotero/storage/MWCANU3A/Goodchild - 2007 - Citizens as sensors the world of volunteered geog.pdf},
  groups       = {Données géographiques},
  langid       = {english},
  shortjournal = {GeoJournal},
  shorttitle   = {Citizens as Sensors},
  url          = {http://link.springer.com/10.1007/s10708-007-9111-y},
  urldate      = {2023-08-17},
}

@Article{Groeger2012,
  author       = {Gröger, Gerhard and Plümer, Lutz},
  journal      = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {{{CityGML}} – {{Interoperable}} Semantic {{3D}} City Models},
  year         = {2012},
  issn         = {0924-2716},
  month        = jul,
  pages        = {12--33},
  volume       = {71},
  abstract     = {CityGML is the international standard of the Open Geospatial Consortium (OGC) for the representation and exchange of 3D city models. It defines the three-dimensional geometry, topology, semantics and appearance of the most relevant topographic objects in urban or regional contexts. These definitions are provided in different, well-defined Levels-of-Detail (multiresolution model). The focus of CityGML is on the semantical aspects of 3D city models, its structures, taxonomies and aggregations, allowing users to employ virtual 3D city models for advanced analysis and visualization tasks in a variety of application domains such as urban planning, indoor/outdoor pedestrian navigation, environmental simulations, cultural heritage, or facility management. This is in contrast to purely geometrical/graphical models such as KML, VRML, or X3D, which do not provide sufficient semantics. CityGML is based on the Geography Markup Language (GML), which provides a standardized geometry model. Due to this model and its well-defined semantics and structures, CityGML facilitates interoperable data exchange in the context of geo web services and spatial data infrastructures. Since its standardization in 2008, CityGML has become used on a worldwide scale: tools from notable companies in the geospatial field provide CityGML interfaces. Many applications and projects use this standard. CityGML is also having a strong impact on science: numerous approaches use CityGML, particularly its semantics, for disaster management, emergency responses, or energy-related applications as well as for visualizations, or they contribute to CityGML, improving its consistency and validity, or use CityGML, particularly its different Levels-of-Detail, as a source or target for generalizations. This paper gives an overview of CityGML, its underlying concepts, its Levels-of-Detail, how to extend it, its applications, its likely future development, and the role it plays in scientific research. Furthermore, its relationship to other standards from the fields of computer graphics and computer-aided architectural design and to the prospective INSPIRE model are discussed, as well as the impact CityGML has and is having on the software industry, on applications of 3D city models, and on science generally.},
  doi          = {10.1016/j.isprsjprs.2012.04.004},
  file         = {/Users/jeremy/Zotero/storage/55WFZUVJ/S0924271612000779.html},
  groups       = {Données d'accessibilité},
  keywords     = {City,GIS,Interoperability,Standards,Three-dimensional,Urban},
  shortjournal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  url          = {https://www.sciencedirect.com/science/article/pii/S0924271612000779},
  urldate      = {2023-08-18},
}

@Article{Biljecki2018,
  author       = {Biljecki, Filip and Kumar, Kavisha and Nagel, Claus},
  journal      = {Open Geospatial Data, Software and Standards},
  title        = {{{CityGML Application Domain Extension}} ({{ADE}}): Overview of Developments},
  year         = {2018},
  issn         = {2363-7501},
  month        = aug,
  number       = {1},
  pages        = {13},
  volume       = {3},
  abstract     = {The Application Domain Extension (ADE) is a built-in mechanism of CityGML to augment its data model with additional concepts required by particular use cases. The goal of this paper is to provide an overview of the ADE mechanism and a literature review of developments since its introduction a decade ago. The discovery of publications found that currently there are 44 ADEs supporting a wide range of applications, but also application-agnostic purposes such as harmonisation with national geographic information standards. We hope this paper to double as a reference material for the developers of new ADEs.},
  doi          = {10.1186/s40965-018-0055-6},
  file         = {/Users/jeremy/Zotero/storage/CY9JWZRF/Biljecki et al. - 2018 - CityGML Application Domain Extension (ADE) overvi.pdf;/Users/jeremy/Zotero/storage/94C28T2B/s40965-018-0055-6.html},
  groups       = {Données d'accessibilité},
  keywords     = {3D city modelling,3D GIS,ADE,CityGML},
  shortjournal = {Open Geospatial Data, Software and Standards},
  shorttitle   = {{{CityGML Application Domain Extension}} ({{ADE}})},
  url          = {https://doi.org/10.1186/s40965-018-0055-6},
  urldate      = {2023-08-18},
}

@Article{Wheeler2020,
  author       = {Wheeler, Bradley and Syzdykbayev, Meirman and Karimi, Hassan A. and Gurewitsch, Raanan and Wang, Yanbo},
  journal      = {Open Geospatial Data, Software and Standards},
  title        = {Personalized Accessible Wayfinding for People with Disabilities through Standards and Open Geospatial Platforms in Smart Cities},
  year         = {2020},
  issn         = {2363-7501},
  month        = dec,
  number       = {1},
  pages        = {2},
  volume       = {5},
  abstract     = {Abstract             Of the many features that smart cities offer, safe and comfortable mobility of pedestrians within the built environment is of particular importance. Safe and comfortable mobility requires that the built environments of smart cities be accessible to all pedestrians, mobility abled and mobility impaired, given their various mobility needs and preferences. This, coupled with advanced technologies such as wayfinding applications, pedestrians can get assistance in finding~the best pathways at different locations and times. Wayfinding applications comprise two components, a database component containing accessibility data, and appropriate algorithms that can utilize accessibility data to meet the mobility needs and preferences of all individuals. While wayfinding applications that provide accessibility on both permanent (e.g., steps) and temporary (e.g., snow) pathways are becoming available, there is a gap in current solutions. There are two elements in the gap, one is that the accessibility data used for finding accessible pathways for people with disabilities are not compliant to the widely agreed upon and available standards, another is that the accessibility data are not available in free and open platforms so that they can be used by developers to develop personalized wayfinding applications and services. To fill this gap, in this paper, we propose a new extension in CityGML with accessibility data. We demonstrate the benefits of the new extension by testing various route options within a city. These route options clearly show the differences between commonly (shortest and fastest) requested and produced pathways and accessible pathways that are feasible and preferred by people who are mobility impaired, such as wheelchair users.},
  doi          = {10.1186/s40965-020-00075-5},
  file         = {/Users/jeremy/Zotero/storage/F3NALV64/Wheeler et al. - 2020 - Personalized accessible wayfinding for people with.pdf},
  groups       = {Données d'accessibilité},
  langid       = {english},
  shortjournal = {Open geospatial data, softw. stand.},
  url          = {https://opengeospatialdata.springeropen.com/articles/10.1186/s40965-020-00075-5},
  urldate      = {2023-08-18},
}

@Online{GTFS,
  file    = {/Users/jeremy/Zotero/storage/NYERDFFC/reference.html},
  groups  = {Données d'accessibilité},
  title   = {Référence - {{General Transit Feed Specification}}},
  url     = {https://gtfs.org/fr/schedule/reference/},
  urldate = {2023-08-18},
}

@Online{NeTEx,
  file    = {/Users/jeremy/Zotero/storage/Q8BGAMMI/netex-cen.eu.html},
  groups  = {Données d'accessibilité},
  langid  = {american},
  title   = {{{NeTEx}} | {{Network Timetable Exchange}}},
  url     = {https://netex-cen.eu/},
  urldate = {2023-08-18},
}

@Article{Ballatore2015,
  author       = {Ballatore, Andrea and Mooney, Peter},
  journal      = {International Journal of Geographical Information Science},
  title        = {Conceptualising the Geographic World: {{The}} Dimensions of Negotiation in Crowdsourced Cartography},
  year         = {2015},
  month        = aug,
  volume       = {29},
  abstract     = {In crowdsourced cartographic projects, mappers coordinate their efforts through online tools to produce digital geospatial artefacts, such as maps and gazetteers, which were once the exclusive territory of professional surveyors and cartographers. In order to produce meaningful and coherent data, contributors need to negotiate a shared conceptualisation that defines the domain concepts, such as road, building, train station, forest, and lake, enabling the communication of geographic knowledge. Considering the OpenStreetMap Wiki website as a case study, this article investigates the nature of this negotiation, driven by a small group of mappers in a context of high contribution inequality. Despite the apparent consensus on the conceptualisation, the negotiation keeps unfolding in a tension between alternative representations, which are often incommensurable, i.e., hard to integrate and reconcile. In this study, we identify six complementary dimensions of incommensurability that recur in the negotiation: (i) ontology, (ii) cartography, (iii) culture and language, (iv) lexical definitions, (v) granularity, and (vi) semantic overload and duplication.},
  doi          = {10.1080/13658816.2015.1076825},
  file         = {/Users/jeremy/Zotero/storage/78K5MJPB/Ballatore et Mooney - 2015 - Conceptualising the geographic world The dimensio.pdf},
  groups       = {VGI},
  shortjournal = {International Journal of Geographical Information Science},
  shorttitle   = {Conceptualising the Geographic World},
}

@Report{WadjomKammegne2021,
  address = {{Clermont-Ferrand, France}},
  author  = {Wadjom Kammegne, Eunice Guédaliah},
  groups  = {Données d'accessibilité},
  school  = {{LIMOS, Université Clermont Auvergne}},
  title   = {Models Transformation and Data Integration in {{OpenStreetMap}} for Accessibility: Feasibility Study from the Geostandard},
  year    = {2021},
}

@Article{Biagi2020,
  author       = {Biagi, L. and Brovelli, M. A. and Stucchi, L.},
  journal      = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Mapping the Accessibility in {{OpenStreetMap}}: {{A}} Comparison of Different Techniques},
  year         = {2020},
  issn         = {2194-9034},
  month        = aug,
  pages        = {229--236},
  volume       = {XLIII-B4-2020},
  abstract     = {Architectural barriers are physical elements that limit the freedom of movement and use of services of a person. The lack of accessibility is one of the physical barriers that most limit people with motor disabilities, as recognised by the World Health Organization. The work aims to identify the optimal methodology to map accessible ways and critical barriers, in order to produce cartographic support for people with motor disabilities. It should also be a tool that allows citizens to report barriers to public authority. The work is part of the ViaLibera?! project, which aims to apply the methodology in the Municipality 9 of the city of Milan. The project is founded by Fondazione di Comunita` Milano; Politecnico di Milano is the scientific partner, while the other partners are associations that represent people with disabilities: Spazio Vita Niguarda Onlus, Ledha Milano and AUS Niguarda Onlus. The mapping elements of interest for the project were identified in collaboration with the other partners, also studying the state of the art. In the framework of Open Street Map, a comparison between different existing mapping techniques was done to select the optimal compromise between rigour and simplicity. In addition, the different techniques must be suitable for the chosen tagging scheme to map accessibility elements. The techniques analysed involve the use of paper maps, Field Papers, and street-level images or applications for smartphones. They are compared to identify the best one.},
  doi          = {10.5194/isprs-archives-XLIII-B4-2020-229-2020},
  file         = {/Users/jeremy/Zotero/storage/ZUETTNN4/Biagi et al. - 2020 - MAPPING THE ACCESSIBILITY IN OPENSTREETMAP A COMP.pdf},
  groups       = {VGI},
  langid       = {english},
  shortjournal = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
  shorttitle   = {{{MAPPING THE ACCESSIBILITY IN OPENSTREETMAP}}},
  url          = {https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/229/2020/},
  urldate      = {2023-08-18},
}

@Article{Mobasheri2017,
  author     = {Mobasheri, Amin and Sun, Yeran and Loos, Lukas and Ali, Ahmed Loai},
  journal    = {Sustainability},
  title      = {Are {{Crowdsourced Datasets Suitable}} for {{Specialized Routing Services}}? {{Case Study}} of {{OpenStreetMap}} for {{Routing}} of {{People}} with {{Limited Mobility}}},
  year       = {2017},
  issn       = {2071-1050},
  month      = jun,
  number     = {6},
  pages      = {997},
  volume     = {9},
  abstract   = {Nowadays, Volunteered Geographic Information (VGI) has increasingly gained attractiveness to both amateur users and professionals. Using data generated from the crowd has become a hot topic for several application domains including transportation. However, there are concerns regarding the quality of such datasets. As one of the most famous crowdsourced mapping platforms, we analyze the fitness for use of OpenStreetMap (OSM) database for routing and navigation of people with limited mobility. We assess the completeness of OSM data regarding sidewalk information. Relevant attributes for sidewalk information such as sidewalk width, incline, surface texture, etc. are considered, and through both extrinsic and intrinsic quality analysis methods, we present the results of fitness for use of OSM data for routing services of disabled persons. Based on empirical results, it is concluded that OSM data of relatively large spatial extents inside all studied cities could be an acceptable region of interest to test and evaluate wheelchair routing and navigation services, as long as other data quality parameters such as positional accuracy and logical consistency are checked and proved to be acceptable. We present an extended version of OSMatrix web service and explore how it is employed to perform spatial and temporal analysis of sidewalk data completeness in OSM. The tool is beneficial for piloting activities, whereas the pilot site planners can query OpenStreetMap and visualize the degree of sidewalk data availability in a certain region of interest. This would allow identifying the areas that data are mostly missing and plan for data collection events. Furthermore, empirical results of data completeness for several OSM data indicators and their potential relation to sidewalk data completeness are presented and discussed. Finally, the article ends with an outlook for future research study in this area.},
  doi        = {10.3390/su9060997},
  file       = {/Users/jeremy/Zotero/storage/RUCBEMSZ/Mobasheri et al. - 2017 - Are Crowdsourced Datasets Suitable for Specialized.pdf;/Users/jeremy/Zotero/storage/A9B3PQLD/997.html},
  groups     = {VGI},
  issue      = {6},
  keywords   = {completeness,data quality,open data,OpenStreetMap,routing,sidewalk},
  langid     = {english},
  publisher  = {{Multidisciplinary Digital Publishing Institute}},
  shorttitle = {Are {{Crowdsourced Datasets Suitable}} for {{Specialized Routing Services}}?},
  url        = {https://www.mdpi.com/2071-1050/9/6/997},
  urldate    = {2022-04-07},
}

@Article{Golledge1998,
  author    = {Golledge, Reginald G. and Klatzky, Roberta L. and Loomis, Jack M. and Speigle, Jon and Tietz, Jerome},
  journal   = {International Journal of Geographical Information Science},
  title     = {A Geographical Information System for a {{GPS}} Based Personal Guidance System},
  year      = {1998},
  issn      = {1365-8816},
  month     = nov,
  number    = {7},
  pages     = {727--749},
  volume    = {12},
  abstract  = {This paper describes the process of building a GIS for use in real time by blind travellers. Initially the components of a Personal Guidance System (PGS) for blind pedestrians are outlined. The location finding and database components of the system are then elaborated. Next follows a discussion of the environmental features likely to be used by blind travellers, and a discussion of the different path following and environmental learning modes that can be activated in the system. Developments such as personalizing the system and accounting for veering are also presented. Finally, possible competing schemes and problems related to the GIS component are examined.},
  doi       = {10.1080/136588198241635},
  groups    = {Navigation in-situ},
  publisher = {{Taylor \& Francis}},
  url       = {https://doi.org/10.1080/136588198241635},
  urldate   = {2023-08-28},
}

@Article{Duh2021,
  author     = {Duh, Ping-Jung and Sung, Yu-Cheng and Chiang, Liang-Yu Fan and Chang, Yung-Ju and Chen, Kuan-Wen},
  journal    = {IEEE Transactions on Multimedia},
  title      = {V-{{Eye}}: {{A Vision-Based Navigation System}} for the {{Visually Impaired}}},
  year       = {2021},
  issn       = {1941-0077},
  pages      = {1567--1580},
  volume     = {23},
  abstract   = {Numerous systems for helping visually impaired people navigate in unfamiliar places have been proposed. However, few can detect and warn about moving obstacles, provide correct orientation in real time, or support navigation between indoor and outdoor spaces. Accordingly, this paper proposes V-Eye, which fulfills these needs by utilizing a novel global localization method (VB-GPS) and image-segmentation techniques to achieve better scene understanding with a single camera. Our experiments establish that the proposed system can reliably provide precise locations and orientation information (with a median error of approximately 0.27 m and 0.95°); detect unpredictable obstacles; and support navigating both within and between indoor and outdoor environments. The results of a user-experience study of V-eye further indicate that it helped the participants not only with navigation, but also improved their awareness of obstacles, enhanced their spatial awareness more generally, and led them to feel more secure and independent while walking.},
  doi        = {10.1109/TMM.2020.3001500},
  eventtitle = {{{IEEE Transactions}} on {{Multimedia}}},
  groups     = {Navigation in-situ},
  keywords   = {Cameras,global localization,Global Positioning System,navigation system,Real-time systems,scene understanding,Servers,Simultaneous localization and mapping,user study,Visualization,Visually impaired},
  shorttitle = {V-{{Eye}}},
}

@Article{Refuveille2012,
  author       = {Refuveille, Pierre},
  journal      = {Revue Francophone d'Orthoptie},
  title        = {Les Nouvelles Technologies à l’usage Des Malvoyants et Des Non-Voyants},
  year         = {2012},
  issn         = {1876-2204},
  month        = jan,
  number       = {1},
  pages        = {29--39},
  volume       = {5},
  abstract     = {Résumé Cet article est le témoignage d’une personne non-voyante sur son utilisation au quotidien des nouvelles technologies. Il fait le point sur les dernières avancées dans les domaines de l’informatique, de la téléphonie et du matériel spécialisé à l’usage des malvoyants et non-voyants~; sur les astuces et trouvailles qui peuvent mener à l’autonomie. Summary This article is the testimony of a blind person regarding his day-to-day use of new technologies. He gives an overview of the latest advances in the areas of IT, telephones and specialised equipment aimed at partially-sighted and blind people as well as of the tips and ideas which can lead to greater autonomy.},
  doi          = {10.1016/j.rfo.2012.03.005},
  groups       = {Navigation in-situ},
  keywords     = {Accessibilité,Accessibility,Autonomie,Autonomy,Blind person,Braille,Electronic magnifier,Internet,Logiciel pour malvoyant,Loupe électronique,Malvoyant,Non-voyant,Partially-sighted person,Screen magnifier,Software for the partially sighted,Speech synthesis,Synthèse vocale,Téléagrandisseur},
  shortjournal = {Revue Francophone d'Orthoptie},
  url          = {https://www.sciencedirect.com/science/article/pii/S1876220412000143},
  urldate      = {2023-08-28},
}

@InProceedings{Connier2018,
  author     = {Connier, Jean and Shi, Hongling and Xu, Shu and Mean, Kun and Vaslin, Philippe and Li, Jian-Jin and De Vaulx, Christophe},
  title      = {La Canne Blanche {{2SEES}} : Concept et Expérimentations},
  year       = {2018},
  month      = oct,
  abstract   = {Les personnes malvoyantes ou aveugles (PMV) préfèrent généralement les cannes blanches aux dispositifs électroniques d'assistance à la mobilité malgré les possibilités supérieures que ces derniers offrent. Cette prédilection pourrait être due en partie à leur manque perçu de robustesse. Le but du projet 2SEES est de créer un dispositif d'assistance à la mobilité intégrant les fonctionnalités nécessaires aux PMV, accessible et facile à utiliser. Ce document présente les apports au concept et le nouveau prototype du 2SEES, ainsi que les résultats de premières expérimentations sur ses fonctionnalités. Mots-clés-2SEES, SEES, assistance aux personnes malvoyantes, dispositif d'assistance aux personnes malvoyantes.},
  file       = {/Users/jeremy/Zotero/storage/TMTYPRXB/Connier et al. - 2018 - La canne blanche 2SEES  concept et expérimentatio.pdf},
  groups     = {Navigation in-situ},
  shorttitle = {La Canne Blanche {{2SEES}}},
}

@InProceedings{Damaschini2005,
  author     = {Damaschini, R and Legras, Richard and Leroux, R and Farcy, René},
  title      = {Electronic {{Travel Aid}} for Blind People},
  year       = {2005},
  address    = {{Lille, France}},
  abstract   = {We have developed two devices which can be fixed on the long cane: the Tom pouce (infrared near-distance indicator) and the Télétact (laser telemeter). In this paper we relate the evaluation of these devices judged in term of improvement of the mobility and orientation of blind people in their travels.},
  eventtitle = {{{AATE}}},
  file       = {/Users/jeremy/Zotero/storage/ZEUD4598/Damaschini et al. - 2022 - Electronic Travel Aid for blind people.pdf},
  groups     = {Navigation in-situ},
}

@Article{Kuriakose2022,
  author     = {Kuriakose, Bineeth and Shrestha, Raju and Sandnes, Frode Eika},
  journal    = {IETE Technical Review},
  title      = {Tools and {{Technologies}} for {{Blind}} and {{Visually Impaired Navigation Support}}: {{A Review}}},
  year       = {2022},
  issn       = {0256-4602},
  month      = jan,
  number     = {1},
  pages      = {3--18},
  volume     = {39},
  abstract   = {The development of navigation tools for people who are visually impaired had become an important concern in the research area of assistive technologies. This paper gives a comprehensive review of different articles published in the area of navigation solutions for people who are visually impaired. Unlike other review papers, this review considers major solutions that work in both the indoor or/and outdoor environments which are based on different technology. From the review, it became clear that the navigation systems proposed for the target users lack some core features that are quite important for independent navigation. Also, there can be instances in which humanitarian conditions also have to be considered in the navigation system design. Based on these findings, a set of recommendations are also given which can be considered in the future design of navigation systems for blind and visually impaired people.},
  doi        = {10.1080/02564602.2020.1819893},
  file       = {/Users/jeremy/Zotero/storage/I9JDVX4R/Kuriakose et al. - 2022 - Tools and Technologies for Blind and Visually Impa.pdf},
  groups     = {Navigation in-situ},
  keywords   = {Accessibility,Assistive technology,Assistive tools,Blind and visually impaired,Navigation,Travel aids},
  publisher  = {{Taylor \& Francis}},
  shorttitle = {Tools and {{Technologies}} for {{Blind}} and {{Visually Impaired Navigation Support}}},
  url        = {https://doi.org/10.1080/02564602.2020.1819893},
  urldate    = {2023-08-28},
}

@Standard{NFS32002_2004,
  organization = {AFNOR},
  title        = {{{NF S32-002}}},
  month        = nov,
  url          = {https://www.boutique.afnor.org/fr-fr/norme/nf-s32002/dispositifs-repetiteurs-de-feux-de-circulation-a-lusage-des-personnes-aveug/fa125183/650},
  year         = {2004},
  groups       = {Navigation in-situ},
  pagetotal    = {19},
}

@InProceedings{Cheraghi2017,
  author     = {Cheraghi, Seyed Ali and Namboodiri, Vinod and Walker, Laura},
  booktitle  = {2017 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications}} ({{PerCom}})},
  title      = {{{GuideBeacon}}: {{Beacon-based}} Indoor Wayfinding for the Blind, Visually Impaired, and Disoriented},
  year       = {2017},
  month      = mar,
  pages      = {121--130},
  abstract   = {There are currently few options for navigational aids for the blind and visually impaired (BVI) in large indoor spaces. Such indoor spaces can be difficult to navigate even for the general sighted population if they are disoriented due to unfamiliarity or other reasons. This paper presents an indoor wayfinding system called GuideBeacon for the blind, visually impaired, and disoriented (BVID) that assists people in navigating between any two points within indoor environments. The GuideBeacon system allows users equipped with smartphones to interact with low cost Bluetooth-based beacons deployed strategically within the indoor space of interest to navigate their surroundings. This paper describes the technical challenges faced in designing such a system, the design decisions made in building the current version of the GuideBeacon system, the solutions developed to meet the technical challenges, and results from the evaluation of the system. Results presented in this paper obtained from field testing GuideBeacon with BVI and sighted participants suggests that it can be used by the BVID for navigation in large indoor spaces independently and effectively.},
  doi        = {10.1109/PERCOM.2017.7917858},
  eventtitle = {2017 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications}} ({{PerCom}})},
  file       = {/Users/jeremy/Zotero/storage/Z9RUDEJ4/Cheraghi et al. - 2017 - GuideBeacon Beacon-based indoor wayfinding for th.pdf},
  groups     = {Navigation in-situ},
  issn       = {2474-249X},
  keywords   = {Navigation,User interfaces},
  shorttitle = {{{GuideBeacon}}},
}

@InProceedings{Ahmetovic2016,
  author     = {Ahmetovic, Dragan and Gleason, Cole and Ruan, Chengxiong and Kitani, Kris and Takagi, Hironobu and Asakawa, Chieko},
  booktitle  = {Proceedings of the 18th {{International Conference}} on {{Human-Computer Interaction}} with {{Mobile Devices}} and {{Services}}},
  title      = {{{NavCog}}: A Navigational Cognitive Assistant for the Blind},
  year       = {2016},
  address    = {{New York, NY, USA}},
  month      = sep,
  pages      = {90--99},
  publisher  = {{Association for Computing Machinery}},
  series     = {{{MobileHCI}} '16},
  abstract   = {Turn-by-turn navigation is a useful paradigm for assisting people with visual impairments during mobility as it reduces the cognitive load of having to simultaneously sense, localize and plan. To realize such a system, it is necessary to be able to automatically localize the user with sufficient accuracy, provide timely and efficient instructions and have the ability to easily deploy the system to new spaces. We propose a smartphone-based system that provides turn-by-turn navigation assistance based on accurate real-time localization over large spaces. In addition to basic navigation capabilities, our system also informs the user about nearby points-of-interest (POI) and accessibility issues (e.g., stairs ahead). After deploying the system on a university campus across several indoor and outdoor areas, we evaluated it with six blind subjects and showed that our system is capable of guiding visually impaired users in complex and unfamiliar environments.},
  doi        = {10.1145/2935334.2935361},
  groups     = {Navigation in-situ},
  isbn       = {978-1-4503-4408-1},
  keywords   = {assistive technologies,bluetooth low-energy beacons,localization,navigation assistance,real world accessibility,turn-by-turn navigation,visual impairments},
  shorttitle = {{{NavCog}}},
  url        = {https://doi.org/10.1145/2935334.2935361},
  urldate    = {2023-08-28},
}

@Article{Shin2022,
  author    = {Shin, Kiyoung and McConville, Ryan and Metatla, Oussama and Chang, Minhye and Han, Chiyoung and Lee, Junhaeng and Roudaut, Anne},
  journal   = {Sensors},
  title     = {Outdoor {{Localization Using BLE RSSI}} and {{Accessible Pedestrian Signals}} for the {{Visually Impaired}} at {{Intersections}}},
  year      = {2022},
  issn      = {1424-8220},
  month     = jan,
  number    = {1},
  pages     = {371},
  volume    = {22},
  abstract  = {One of the major challenges for blind and visually impaired (BVI) people is traveling safely to cross intersections on foot. Many countries are now generating audible signals at crossings for visually impaired people to help with this problem. However, these accessible pedestrian signals can result in confusion for visually impaired people as they do not know which signal must be interpreted for traveling multiple crosses in complex road architecture. To solve this problem, we propose an assistive system called CAS (Crossing Assistance System) which extends the principle of the BLE (Bluetooth Low Energy) RSSI (Received Signal Strength Indicator) signal for outdoor and indoor location tracking and overcomes the intrinsic limitation of outdoor noise to enable us to locate the user effectively. We installed the system on a real-world intersection and collected a set of data for demonstrating the feasibility of outdoor RSSI tracking in a series of two studies. In the first study, our goal was to show the feasibility of using outdoor RSSI on the localization of four zones. We used a k-nearest neighbors (kNN) method and showed it led to 99.8\% accuracy. In the second study, we extended our work to a more complex setup with nine zones, evaluated both the kNN and an additional method, a Support Vector Machine (SVM) with various RSSI features for classification. We found that the SVM performed best using the RSSI average, standard deviation, median, interquartile range (IQR) of the RSSI over a 5 s window. The best method can localize people with 97.7\% accuracy. We conclude this paper by discussing how our system can impact navigation for BVI users in outdoor and indoor setups and what are the implications of these findings on the design of both wearable and traffic assistive technology for blind pedestrian navigation.},
  doi       = {10.3390/s22010371},
  file      = {/Users/jeremy/Zotero/storage/9UGZ7XCJ/Shin et al. - 2022 - Outdoor Localization Using BLE RSSI and Accessible.pdf},
  groups    = {Navigation in-situ},
  issue     = {1},
  keywords  = {BLE RSSI,localization at an intersection,pedestrian navigation,visually impaired},
  langid    = {english},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  url       = {https://www.mdpi.com/1424-8220/22/1/371},
  urldate   = {2023-08-28},
}

@Thesis{Ducasse2017,
  abstract    = {En dépit de leur omniprésence et de leur rôle essentiel dans nos vies professionnelles et personnelles, les représentations graphiques, qu'elles soient numériques ou sur papier, ne sont pas accessibles aux personnes déficientes visuelles car elles ne fournissent pas d'informations tactiles. Par ailleurs, les inégalités d'accès à ces représentations ne cessent de s'accroître ; grâce au développement de représentations graphiques dynamiques et disponibles en ligne, les personnes voyantes peuvent non seulement accéder à de grandes quantités de données, mais aussi interagir avec ces données par le biais de fonctionnalités avancées (changement d'échelle, sélection des données à afficher, etc.). En revanche, pour les personnes déficientes visuelles, les techniques actuellement utilisées pour rendre accessibles les cartes et les diagrammes nécessitent l'intervention de spécialistes et ne permettent pas la création de représentations interactives. Cependant, les récentes avancées dans le domaine de l'adaptation automatique de contenus laissent entrevoir, dans les prochaines années, une augmentation de la quantité de contenus adaptés. Cette augmentation doit aller de pair avec le développement de dispositifs utilisables et abordables en mesure de supporter l'affichage de représentations interactives et rapidement modifiables, tout en étant accessibles aux personnes déficientes visuelles. Certains prototypes de recherche s'appuient sur une représentation numérique seulement : ils peuvent être instantanément modifiés mais ne fournissent que très peu de retour tactile, ce qui rend leur exploration complexe d'un point de vue cognitif et impose de fortes contraintes sur le contenu. D'autres prototypes s'appuient sur une représentation numérique et physique : bien qu'ils puissent être explorés tactilement, ce qui est un réel avantage, ils nécessitent un support tactile qui empêche toute modification rapide. Quant aux dispositifs similaires à des tablettes Braille, mais avec des milliers de picots, leur coût est prohibitif. L'objectif de cette thèse est de pallier les limitations de ces approches en étudiant comment développer des cartes et diagrammes interactifs physiques, modifiables et abordables. Pour cela, nous nous appuyons sur un type d'interface qui a rarement été étudié pour des utilisateurs déficients visuels : les interfaces tangibles, et plus particulièrement les interfaces tangibles sur table. Dans ces interfaces, des objets physiques représentent des informations numériques et peuvent être manipulés par l'utilisateur pour interagir avec le système, ou par le système lui-même pour refléter un changement du modèle numérique - on parle alors d'interfaces tangibles sur tables animées, ou actuated. Grâce à la conception, au développement et à l'évaluation de trois interfaces tangibles sur table (les Tangible Reels, la Tangible Box et BotMap), nous proposons un ensemble de solutions techniques répondant aux spécificités des interfaces tangibles pour des personnes déficientes visuelles, ainsi que de nouvelles techniques d'interaction non-visuelles, notamment pour la reconstruction d'une carte ou d'un diagramme et l'exploration de cartes de type " Pan \& Zoom ". D'un point de vue théorique, nous proposons aussi une nouvelle classification pour les dispositifs interactifs accessibles.},
  author      = {Ducasse, Julie},
  editora     = {Jouffrais, Christophe and Macé, Marc},
  editoratype = {collaborator},
  groups      = {Maquettes},
  keywords    = {Accessibilité,Accessibility,Cartes -- Lecture,Déficience visuelle,Graphical representations,Handicapés visuels -- Cartes,Interaction homme-robot,Interaction tangible,Représentations graphiques,Tangible interaction,Visual impairment},
  month       = oct,
  school      = {{Toulouse 3}},
  title       = {Tabletop Tangible Maps and Diagrams for Visually Impaired Users},
  type        = {These de doctorat},
  url         = {https://www.theses.fr/2017TOU30197},
  urldate     = {2023-08-29},
  year        = {2017},
}

@InProceedings{Mulet2020,
  author       = {Mulet, Julie and Dimitrov, Lachezar and Bartolucci, Anna and Raynal, Mathieu and Tartas, Val{\'e}rie and Ducasse, Julie and Mac{\'e}, Marc J.-M. and Oriola, Bernard and Lemari{\'e}, Julie and Jouffrais, Christophe},
  booktitle    = {{11{\`e}me conf{\'e}rence de l'IFRATH sur les technologies d'assistance: Technologies pour l'autonomie et l'inclusion (Handicap 2020)}},
  title        = {{{\'E}valuation d'un dispositif audio-tangible pour l'apprentissage spatial chez les enfants d{\'e}ficients visuels}},
  year         = {2020},
  address      = {Paris (en distanciel), France},
  editor       = {C. Jost and G. Uzan},
  month        = Nov,
  organization = {{Institut F{\'e}d{\'e}ratif de Recherche sur les Aides Techniques pour personnes Handicap{\'e}es (IFRATH)}},
  pages        = {157-162},
  publisher    = {{IFRATH}},
  volume       = {Session 6 : Outils p{\'e}dagogiques pour personnes aveugles},
  groups       = {Maquettes},
  hal_id       = {hal-02926462},
  hal_version  = {v1},
  keywords     = {accessibilit{\'e} ; carte interactive ; d{\'e}ficiencevisuelle ; navigation autonome ; enseignement sp{\'e}cialis{\'e}},
  pdf          = {https://hal.science/hal-02926462/file/H2020_paper_34.pdf},
  url          = {https://hal.science/hal-02926462},
}

@Report{Thome2021,
  author = {Thomé, Cathy},
  groups = {Son},
  month  = sep,
  school = {{LIMOS, Université Clermont Auvergne}},
  title  = {Faciliter et Sécuriser Les Déplacements Extérieurs Pour Les Personnes En Situation de Handicap Visuel via Un Entrainement Auditif},
  year   = {2021},
}

@InProceedings{Guerreiro2017,
  author     = {Guerreiro, João and Ahmetovic, Dragan and Kitani, Kris M. and Asakawa, Chieko},
  booktitle  = {Proceedings of the 19th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  title      = {Virtual {{Navigation}} for {{Blind People}}: {{Building Sequential Representations}} of the {{Real-World}}},
  year       = {2017},
  address    = {{Baltimore Maryland USA}},
  month      = oct,
  pages      = {280--289},
  publisher  = {{ACM}},
  doi        = {10.1145/3132525.3132545},
  eventtitle = {{{ASSETS}} '17: {{The}} 19th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  groups     = {Navigation},
  isbn       = {978-1-4503-4926-0},
  langid     = {english},
  shorttitle = {Virtual {{Navigation}} for {{Blind People}}},
  url        = {https://dl.acm.org/doi/10.1145/3132525.3132545},
  urldate    = {2023-07-26},
}

@Article{Arabsheibani2023,
  author     = {Arabsheibani, Reza and Hamzei, Ehsan and Amoozandeh, Kimia and Winter, Stephan and Tomko, Martin},
  journal    = {AGILE: GIScience Series},
  title      = {From {{Floorplan}} to {{Navigation Concepts}}: {{Automatic Generation}} of {{Text-based Games}}},
  year       = {2023},
  month      = jun,
  pages      = {1--15},
  volume     = {4},
  abstract   = {Text-based games are environments in which defining the world, the representation of the world to the player (hereafter, agent) and agent interactions with the environment are all through text. Text-based games expose abstract, executable representations of indoor spaces through verbally referenced concepts. Yet, the ability of text-based games to represent indoor environments of real-world complexity is currently limited due to insufficient support for complex space decomposition and space interaction concepts. This paper suggests a procedure to automate the mapping of real-world geometric floorplan information into text-based game environment concepts, using the Microsoft TextWorld game platform as a case. To capture the complexities of indoor spaces, we enrich existing TextWorld concepts supported by theoretical navigation concepts.We first decompose indoor spaces using skeletonization, and then identify formal space concepts and their relationships. We further enhance the spectrum of supported agent interactions with an extended grammar, including egocentric navigation instructions. We demonstrate and discuss these new capabilities in an evacuation scenario. Our implementation extends the capabilities of TextWorld to provide a research testbed for spatial research, including symbolic spatial modelling, interaction with indoor spaces, and agent-based machine learning and language processing tasks.},
  doi        = {10.5194/agile-giss-4-2-2023},
  file       = {/Users/jeremy/Zotero/storage/EMGYTUY5/Arabsheibani et al. - 2023 - From Floorplan to Navigation Concepts Automatic G.pdf},
  groups     = {Navigation},
  keywords   = {indoor space,space concepts,symbolic modelling,text-based games},
  langid     = {english},
  publisher  = {{Copernicus GmbH}},
  shorttitle = {From {{Floorplan}} to {{Navigation Concepts}}},
  url        = {https://agile-giss.copernicus.org/articles/4/2/2023/},
  urldate    = {2023-08-29},
}

@Article{Brazier2008,
  author   = {Brazier, Jodi},
  journal  = {Vision Rehabilitation International},
  title    = {The {{Benefits}} of {{Using Echolocation}} to {{Safely Navigate Through}} the {{Environment}}},
  year     = {2008},
  month    = jan,
  number   = {1},
  pages    = {46--51},
  volume   = {1},
  abstract = {Abstract This study investigated participant use of echolocation skills. Specifically participants were asked to describe what methods they used to generate sound, and what echolocation assisted them to do in terms of their orientation and mobility (O\&amp;M). Contrary to previous research findings, it was found that most participants preferred to use cane tapping to generate sound. Possible reasons for this finding are discussed. Participants reported that echolocation assisted them to self-orientate and detect, locate and differentiate between objects in the environment.},
  doi      = {10.21307/ijom-2008-005},
  file     = {/Users/jeremy/Zotero/storage/ZNKYVLVC/Brazier - 2008 - The Benefits of Using Echolocation to Safely Navig.pdf},
  groups   = {Déficience visuelle},
  langid   = {english},
  url      = {https://sciendo.com/article/10.21307/ijom-2008-005?tab=figures-tables},
  urldate  = {2023-08-29},
}

@Article{Jacobson1998,
  author       = {Jacobson, R. Dan},
  journal      = {Journal of Environmental Psychology},
  title        = {Cognitive {{Mapping Without Sight}}: Four Preliminary Studies of Spatial Learning},
  year         = {1998},
  issn         = {0272-4944},
  month        = sep,
  number       = {3},
  pages        = {289--305},
  volume       = {18},
  abstract     = {This paper illustrates the application of cognitive mapping to people with visual impairments and blindness. It gives perspectives on past research, outlines ongoing research, highlights some of the methodological and validity issues arising from this research, and discusses the movement of theory into practice. The findings of three small preliminary studies have been reported, as part of continuing research into the cognitive mapping abilities of blind or visually impaired people. These studies have highlighted the need to use multiple, mutually supportive tests to assess cognitive map knowledge. In light of these findings and the need to move theory into practice, a current research project is outlined. This project seeks to use the knowledge gained from the three projects to design and implement an auditory hypermap system to aid wayfinding and the spatial learning of an area. Finally an agenda for applied research is presented.},
  doi          = {10.1006/jevp.1998.0098},
  groups       = {Cognition spatiale},
  shortjournal = {Journal of Environmental Psychology},
  shorttitle   = {Cognitive {{Mapping Without Sight}}},
  url          = {https://www.sciencedirect.com/science/article/pii/S0272494498900986},
  urldate      = {2023-08-29},
}

@InCollection{Giudice2010,
  author = {Giudice, Nicholas and Long, R.G.},
  title  = {Establishing and {{Maintaining Orientation}} for {{Orientation}} and {{Mobility}}},
  year   = {2010},
  month  = jan,
  pages  = {45--62},
  file   = {/Users/jeremy/Zotero/storage/JHZ3NU7A/Giudice et Long - 2010 - Establishing and Maintaining Orientation for Orien.pdf},
  groups = {Cognition spatiale},
}

@InCollection{Denis2023,
  author = {Denis, Lisa and Kalsron, Jérémy and Favreau, Jean-Marie},
  title  = {Représenter l’espace Urbain Pour Les Personnes Concernées Par Le Handicap Visuel},
  year   = {2023},
  groups = {Cognition spatiale},
}

@InProceedings{Touya2014,
  author    = {Touya, Guillaume and Girres, Jean-François},
  booktitle = {17th {{ICA Workshop}} on {{Map Generalisation}} and {{Multiple Representation}}},
  title     = {Generalising {{Unusual Map Themes}} from {{OpenStreetMap}}},
  year      = {2014},
  address   = {{Vienna, Austria}},
  month     = sep,
  file      = {/Users/jeremy/Zotero/storage/85K7ZJJN/Touya et Girres - 2014 - Generalising Unusual Map Themes from OpenStreetMap.pdf},
  groups    = {Manipulation des données},
  keywords  = {Map generalisation,VGI},
  url       = {https://hal.archives-ouvertes.fr/hal-02280105},
  urldate   = {2022-04-04},
}

@Article{Favreau2022,
  author       = {Favreau, Jean-Marie and Kalsron, Jérémy},
  journal      = {AGILE: GIScience Series},
  title        = {What Are Intersections for Pedestrian Users?},
  year         = {2022},
  issn         = {2700-8150},
  month        = jun,
  pages        = {1--15},
  volume       = {3},
  abstract     = {The increase of accessibility and pedestrian data in geographic databases such as OpenStreetMap brings with it the possibility to find a number of applications for pedestrian users.},
  doi          = {10.5194/agile-giss-3-4-2022},
  file         = {/Users/jeremy/Zotero/storage/WB9Y635J/Favreau et Kalsron - 2022 - What are intersections for pedestrian users.pdf},
  groups       = {Données d'accessibilité},
  langid       = {english},
  shortjournal = {AGILE GIScience Ser.},
  url          = {https://agile-giss.copernicus.org/articles/3/4/2022/},
  urldate      = {2023-06-15},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectoryLatex-jeremy-MacBook-Air-de-Jeremy.local:/Users/jeremy/Developpement/these/chapitres;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Cartes tactiles\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Cartes audio-tactiles\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Cartographie sonore\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Description verbale\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Spécifique carrefour\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Dispositifs d'aide\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Navigation in-situ\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Données géographiques\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Données d'accessibilité\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Manipulation des données\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:VGI\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Déficience visuelle\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Cognition spatiale\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Réalité virtuelle\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Maquettes\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Navigation\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Son\;0\;1\;0x8a8a8aff\;\;\;;
}
